Microsoft (R) Macro Assembler (x64) Version 14.44.35207.1   05/26/25 13:24:26
ui512md.asm						     Page 1 - 1


				;
				;			ui512md
				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;
				;			File:			ui512md.asm
				;			Author:			John G. Lynch
				;			Legal:			Copyright @2024, per MIT License below
				;			Date:			June 20, 2024
				;
				;			Notes:
				;				ui512 is a small project to provide basic operations for a variable type of unsigned 512 bit integer.
				;
				;				ui512a provides basic operations: zero, copy, compare, add, subtract.
				;				ui512b provides basic bit-oriented operations: shift left, shift right, and, or, not, least significant bit and most significant bit.
				;               ui512md provides multiply and divide.
				;
				;				It is written in assembly language, using the MASM (ml64) assembler provided as an option within Visual Studio.
				;				(currently using VS Community 2022 17.9.6)
				;
				;				It provides external signatures that allow linkage to C and C++ programs,
				;				where a shell/wrapper could encapsulate the methods as part of an object.
				;
				;				It has assembly time options directing the use of Intel processor extensions: AVX4, AVX2, SIMD, or none:
				;				(Z (512), Y (256), or X (128) registers, or regular Q (64bit)).
				;
				;				If processor extensions are used, the caller must align the variables declared and passed
				;				on the appropriate byte boundary (e.g. alignas 64 for 512)
				;
				;				This module is very light-weight (less than 1K bytes) and relatively fast,
				;				but is not intended for all processor types or all environments. 
				;
				;				Use for private (hobbyist), or instructional, or as an example for more ambitious projects is all it is meant to be.
				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;
				;			MIT License
				;
				;			Copyright (c) 2024 John G. Lynch
				;
				;				Permission is hereby granted, free of charge, to any person obtaining a copy
				;				of this software and associated documentation files (the "Software"), to deal
				;				in the Software without restriction, including without limitation the rights
				;				to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
				;				copies of the Software, and to permit persons to whom the Software is
				;				furnished to do so, subject to the following conditions:
				;
				;				The above copyright notice and this permission notice shall be included in all
				;				copies or substantial portions of the Software.
				;
				;				THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
				;				IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
				;				FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
				;				AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
				;				LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
				;				OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
				;				SOFTWARE.
				;
				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------

								INCLUDE			ui512aMacros.inc
			      C .NOLIST
			      C ;
			      C ;			ui512aMacros
			      C ;
			      C ;			File:			ui512aMacros.inc
			      C ;			Author:			John G. Lynch
			      C ;			Legal:			Copyright @2024, per MIT License below
			      C ;			Date:			May 13, 2024
			      C ;
			      C ;			Notes:
			      C ;				ui512 is a small project to provide basic operations for a variable type of unsigned 512 bit integer.
			      C ;
			      C ;				ui512a provides basic operations: zero, copy, compare, add, subtract.
			      C ;				ui512b provides basic bit-oriented operations: shift left, shift right, and, or, not, least significant bit and most significant bit.
			      C ;               ui512md provides multiply and divide.
			      C ;
			      C ;				It is written in assembly language, using the MASM (ml64) assembler provided as an option within Visual Studio.
			      C ;				(currently using VS Community 2022 17.9.6)
			      C ;
			      C ;				It provides external signatures that allow linkage to C and C++ programs,
			      C ;				where a shell/wrapper could encapsulate the methods as part of an object.
			      C ;
			      C ;				It has assembly time options directing the use of Intel processor extensions: AVX4, AVX2, SIMD, or none:
			      C ;				(Z (512), Y (256), or X (128) registers, or regular Q (64bit)).
			      C ;
			      C ;				If processor extensions are used, the caller must align the variables declared and passed
			      C ;				on the appropriate byte boundary (e.g. alignas 64 for 512)
			      C ;
			      C ;				This module is very light-weight (less than 2K bytes) and relatively fast,
			      C ;				but is not intended for all processor types or all environments. 
			      C ;
			      C ;				Use for private (hobbyist), or instructional, or as an example for more ambitious projects is all it is meant to be.
			      C ;
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C ;
			      C ;			MIT License
			      C ;
			      C ;			Copyright (c) 2024 John G. Lynch
			      C ;
			      C ;				Permission is hereby granted, free of charge, to any person obtaining a copy
			      C ;				of this software and associated documentation files (the "Software"), to deal
			      C ;				in the Software without restriction, including without limitation the rights
			      C ;				to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
			      C ;				copies of the Software, and to permit persons to whom the Software is
			      C ;				furnished to do so, subject to the following conditions:
			      C ;
			      C ;				The above copyright notice and this permission notice shall be included in all
			      C ;				copies or substantial portions of the Software.
			      C ;
			      C ;				THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
			      C ;				IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
			      C ;				FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
			      C ;				AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
			      C ;				LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
			      C ;				OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
			      C ;				SOFTWARE.
			      C ;
			      C .LIST
			      C IFNDEF			ui512aMacros_INC
 = 1			      C ui512aMacros_INC EQU			<1>
			      C 
			      C ;           header file equivalent extern declarations
			      C ;			EXTERN "C" signatures (from ui512a.asm)
			      C 
			      C ;	// void zero_u ( u64* destarr ); 
			      C ;	// fill supplied 512bit (8 QWORDS) with zero
			      C EXTERNDEF		zero_u:PROC
			      C 
			      C ;	// void copy_u ( u64* destarr, u64* srcarr );
			      C ;	// copy supplied 512bit (8 QWORDS) source to supplied destination
			      C EXTERNDEF		copy_u:PROC
			      C 
			      C ;	// void set_uT64 ( u64* destarr, u64 value );
			      C ;	// set supplied destination 512 bit to supplied u64 value
			      C EXTERNDEF		set_uT64:PROC
			      C 
			      C ;	// s16 compare_u ( u64* lh_op, u64* rh_op );
			      C ;	// compare supplied 512bit (8 QWORDS) LH operand to supplied RH operand
			      C ;	// returns: (0) for equal, -1 for less than, 1 for greater than (logical, unsigned compare)
			      C EXTERNDEF		compare_u:PROC
			      C 
			      C ;	// s16 compare_uT64 ( u64* lh_op, u64 rh_op );
			      C ;	// compare supplied 512bit (8 QWORDS) LH operand to supplied 64bit RH operand (value)
			      C ;	// returns: (0) for equal, -1 for less than, 1 for greater than (logical, unsigned compare)
			      C EXTERNDEF		compare_uT64:PROC
			      C 
			      C ;	// s16 add_u ( u64* sum, u64* addend1, u64* addend2 );
			      C ;	// add supplied 512bit (8 QWORDS) sources, place in supplied destination
			      C ;	// returns: zero for no carry, 1 for carry (overflow)
			      C EXTERNDEF		add_u:PROC
			      C 
			      C ;	// s16 add_uT64 ( u64* sum, u64* addend1, u64 addend2 );
			      C ;	// add 64bit QWORD (value) to supplied 512bit (8 QWORDS), place in supplied destination
			      C ;	// returns: zero for no carry, 1 for carry (overflow)
			      C EXTERNDEF		add_uT64:PROC
			      C 
			      C ;	// s16 sub_u ( u64* difference, u64* left operand, u64* right operand );
			      C ;	// subtract supplied 512bit (8 QWORDS) RH OP from LH OP giving difference in destination
			      C ;	// returns: zero for no borrow, 1 for borrow (underflow)
			      C EXTERNDEF		sub_u:PROC
			      C 
			      C ;	// s16 sub_uT64( u64* difference, u64* left operand, u64 right operand );
			      C ;	// subtract supplied 64 bit right hand (64 bit value) op from left hand (512 bit) giving difference
			      C ;	// returns: zero for no borrow, 1 for borrow (underflow)
			      C EXTERNDEF		sub_uT64:PROC
			      C 
			      C ;			Configuration choices
 = 00000001		      C __UseZ			EQU				1									; Use AVX4 processor features (512 bit registers and instructions)
 = 00000000		      C __UseY			EQU				0									; Use AVX2 processor features (256 bit registers and instructions)
 = 00000000		      C __UseX			EQU				0									; Use SIMD/SSE processor features (128 bit registers and instructions)
 = 00000001		      C __UseQ			EQU				1									; Do not use extensions, use standard x64 bit registers and instructions
			      C ;
 = 00000000		      C __CheckAlign	EQU				0									; User is expected to pass arguments aligned on 64 byte boundaries, 
			      C 																	; This setting enforces that with a check. It should not be necessary, but included to help debugging
			      C 
			      C ;           Some coding shortcuts
 = ZMMWORD PTR		      C ZM_PTR			EQU				ZMMWORD PTR
 = YMMWORD PTR		      C YM_PTR			EQU				YMMWORD PTR
 = XMMWORD PTR		      C XM_PTR			EQU				XMMWORD PTR
 = QWORD PTR		      C Q_PTR			EQU				QWORD PTR
 = DWORD PTR		      C D_PTR			EQU				DWORD PTR
 = WORD PTR		      C W_PTR			EQU				WORD PTR
 = BYTE PTR		      C B_PTR			EQU				BYTE PTR
 = DWORD BCST		      C m32BCST			EQU				DWORD BCST
 = QWORD BCST		      C m64BCST			EQU				QWORD BCST
			      C 
			      C ;			mask codes (for compares using instructions like VPCMPUQ)
 = 00000000		      C CPEQ			EQU				0
 = 00000001		      C CPLT			EQU				1
 = 00000002		      C CPLE			EQU				2
 = 00000003		      C CPFALSE			EQU				3
 = 00000004		      C CPNE			EQU				4
 = 00000005		      C CPGE			EQU				5
 = 00000006		      C CPGT			EQU				6
 = 00000007		      C CPTRUE			EQU				7
			      C 
			      C ;			Mask values (for k reg) used to select particulare QWORDS from X, Y, or Z simd regs
 = 00000001		      C MaskBit0		EQU				B_PTR [ 00000001b ]
 = 00000002		      C MaskBit1		EQU				B_PTR [ 00000010b ]
 = 00000004		      C MaskBit2		EQU				B_PTR [ 00000100b ]
 = 00000008		      C MaskBit3		EQU				B_PTR [ 00001000b ]
 = 00000010		      C MaskBit4		EQU				B_PTR [ 00010000b ]
 = 00000020		      C MaskBit5		EQU				B_PTR [ 00100000b ]
 = 00000040		      C MaskBit6		EQU				B_PTR [ 01000000b ]
 = 00000080		      C MaskBit7		EQU				B_PTR [ 10000000b ]
			      C 
			      C ;			Another way to get masks
			      C kMask			RECORD			b8:1, b7:1, b6:1, b5:1, b4:1, b3:1, b2:1, b1:1, b0:1
			      C ;==========================================================================================
			      C ;           Notes on x64 calling conventions        aka "fast call"
			      C ; ref: https://learn.microsoft.com/en-us/cpp/build/x64-calling-convention?view=msvc-170
			      C ; The first four parameters are passed in registers: RCX, RDX, R8, R9 if integer or address
			      C ; if floating point XMM0L, XMM1L, XMM2L, XMM3L
			      C ; return (if any) is in EAX
			      C ;===========================================================================================
			      C ;
			      C ;===========================================================================================
			      C ; RAX, RCX, RDX, R8, R9, R10, R11 are considered volatile, and do not need to be saved
			      C ; XMM0, YMM0, ZMM0 and  ..1, ..2, ..3, ..4, and ..5 are considered volatile,
			      C ;	and do not need to be saved
			      C ;  ZMM16 to ZMM31: volatile, also do not need to be zeroed to resume full clock speeds
			      C ;
			      C ; R12, R13, R14, R15, RDI, RSI, RBX, RBP, RSP are non-volatile and if used, must be restored
			      C ; XMM, YMM, and ZMM ..6 thru 15 are non-volatile and if used, must be restored
			      C ;
			      C ; A "leaf" function is one that does not call and does not change non volatile registers
			      C ; leaf functionss therefore do not need frame, prolog or epilog
			      C ;
			      C ;===========================================================================================
			      C 
			      C 
			      C ;===========================================================================================
			      C ;          Local macros
			      C ;===========================================================================================
			      C 
			      C ;
			      C ;			Test passed variable addresses for 64 byte alignment
			      C ;			Note: Better performance if this is off, but for debugging, maybe have it on
			      C ;
			      C CheckAlign		MACRO			Raddr
			      C 				LOCAL			ok
			      C 	IF	__CheckAlign
			      C 				TEST			Raddr, 63							; Is specified param aligned 64?
			      C 				JZ				ok									; Yes, passes test, continue
			      C 				INT				0									; No? fails, break (can substitute other exception handling)
			      C ok:
			      C 	ENDIF
			      C 				ENDM
			      C 
			      C MemConstants	MACRO
			      C ;
			      C zeroQ			DQ				0
			      C mskHex100		DD				0100h
			      C ;		Return codes commonly used.			
			      C ret0			DD				0								
			      C ret1			DD				1
			      C ret_1			DD				-1
			      C ;		Masks commonly used
			      C mskAll8			DB				255
			      C mskB0			DB				1
			      C mskB1			DB				2
			      C mskB2			DB				4
			      C mskB3			DB				8
			      C mskB4			DB				16
			      C mskB5			DB				32
			      C mskB6			DB				64
			      C mskB7			DB				128
			      C ;
			      C 				ALIGN			8									; realign
			      C 				ENDM
			      C 
			      C ;
			      C ;			Zero a 512 bit destination, conditional assembly based on configuration parameters
			      C ;
			      C Zero512			MACRO			dest:REQ, expand:=<" ">
			      C 	IF		__UseZ AND expand NE "Q"
			      C 				CheckAlign		dest
			      C 				VPXORQ			ZMM31, ZMM31, ZMM31
			      C 				VMOVDQA64		ZM_PTR [ dest ], ZMM31
			      C 	ELSEIF	__UseY AND expand NE "Q"
			      C 				CheckAlign		dest
			      C 				VPXORQ			YMM4, YMM4, YMM4
			      C 				FOR				idx, < 0, 4 >
			      C 				VMOVDQA64		YM_PTR [ dest + idx * 8 ], YMM4
			      C 				ENDM
			      C 	ELSEIF	__UseX AND expand NE "Q"
			      C 				CheckAlign		dest
			      C 				PXOR			XMM4, XMM4
			      C 				FOR				idx, < 0, 2, 4, 6 >
			      C 				MOVDQA			XM_PTR [ dest + idx * 8 ], XMM4
			      C 				ENDM		
			      C 	ELSE
			      C 				XOR				RAX, RAX
			      C 				FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			      C 				MOV				Q_PTR [ dest + idx * 8 ], RAX
			      C 				ENDM
			      C 	ENDIF
			      C 				ENDM
			      C 
			      C 
			      C ;
			      C ;			Copy a 512 bit source to destination, conditional assembly based on configuration parameters
			      C ;
			      C Copy512			MACRO			dest:REQ, src:REQ, expand:=<" ">
			      C 	IF		__UseZ AND expand NE "Q"
			      C 				CheckAlign		dest
			      C 				CheckAlign		src
			      C 				VMOVDQA64		ZMM31, ZM_PTR [ src ]
			      C 				VMOVDQA64		ZM_PTR [ dest ], ZMM31
			      C 	ELSEIF	__UseY AND expand NE "Q"
			      C 				CheckAlign		dest
			      C 				CheckAlign		src
			      C 				VMOVDQA64		YMM4, YM_PTR [ src + 0 * 8 ]
			      C 				VMOVDQA64		YM_PTR [ dest + 0 * 8 ], YMM4	; alternate ymm regs in case pipeline can execute next without waiting for this.
			      C 				VMOVDQA64		YMM5, YM_PTR [ src + 4 * 8 ]
			      C 				VMOVDQA64		YM_PTR [ dest + 4 * 8 ], YMM5
			      C 	ELSEIF	__UseX AND expand NE "Q"
			      C 
			      C 				CheckAlign		dest
			      C 				CheckAlign		src
			      C 				MOVDQA			XMM4, XM_PTR [ src + 0 * 8 ]
			      C 				MOVDQA			XM_PTR [ dest + 0 * 8 ], XMM4
			      C 				MOVDQA			XMM3, XM_PTR [ src + 2 * 8 ]
			      C 				MOVDQA			XM_PTR [ dest + 2 * 8 ], XMM3
			      C 				MOVDQA			XMM4, XM_PTR [ src + 4 * 8 ]
			      C 				MOVDQA			XM_PTR [ dest + 4 * 8 ], XMM4
			      C 				MOVDQA			XMM3, XM_PTR [ src + 6 * 8 ]
			      C 				MOVDQA			XM_PTR [ dest + 6 * 8 ], XMM3
			      C 	ELSE
			      C 				FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
			      C 				MOV				RAX, Q_PTR [ src + idx * 8 ]
			      C 				MOV				Q_PTR [ dest + idx * 8 ], RAX
			      C 				ENDM
			      C 	ENDIF
			      C 				ENDM
			      C 
			      C ;
			      C ;			Get a GP reg QWORD from within a Z register as specified by mask
			      C ;			Note: RAX, ZMM0 and k1 are used and not restored
			      C ;			Example usage: GetZatIdx R11, ZMM1, MaskBit2 or SetZatIdx ZMM1, R12, [ R9 ]  (where R9 is a bit mask, not an integer index)
			      C ;			Note: These are req to reg ops; no memory fetches (other than instructions from pipeline)
			      C ;
			      C GetZatMask		MACRO			dest, src, mask
			      C 				LEA				RAX,  mask
			      C 				KMOVB			k1, RAX
			      C 				VPCOMPRESSQ		ZMM0 {k1}{z}, src
			      C 				VMOVQ			dest, XMM0
			      C 				ENDM
			      C 
			      C ;
			      C ;			Set a GP Reg QWORD within a Z register as specified by mask
			      C ;			Note: RAX and k1 are used and not restored
			      C ;			Example usage: SetZatIdx ZMM1, R8, MaskBit2
			      C ;			Note: These are req to reg ops; no memory fetches (other than instructions from pipeline)
			      C ;
			      C SetZatMask		MACRO			dest, src, mask
			      C 				LEA				RAX, mask
			      C 				KMOVB			k1, RAX
			      C 				VPBROADCASTQ 	dest {k1}, src
			      C 				ENDM
			      C ENDIF
			      C 
								INCLUDE			ui512bMacros.inc
			      C .nolist
			      C ;
			      C ;			ui512bMacros
			      C ;
			      C ;			File:			ui512bMacros.inc
			      C ;			Author:			John G. Lynch
			      C ;			Legal:			Copyright @2024, per MIT License below
			      C ;			Date:			June 11, 2024
			      C ;
			      C ;			Notes:
			      C ;				ui512 is a small project to provide basic operations for a variable type of unsigned 512 bit integer.
			      C ;				The basic operations: zero, copy, compare, add, subtract.
			      C ;               Other optional modules provide bit ops and multiply / divide.
			      C ;				It is written in assembly language, using the MASM (ml64) assembler provided as an option within Visual Studio.
			      C ;				(currently using VS Community 2022 17.9.6)
			      C ;				It provides external signatures that allow linkage to C and C++ programs,
			      C ;				where a shell/wrapper could encapsulate the methods as part of an object.
			      C ;				It has assembly time options directing the use of Intel processor extensions: AVX4, AVX2, SIMD, or none:
			      C ;				(Z (512), Y (256), or X (128) registers, or regular Q (64bit)).
			      C ;				If processor extensions are used, the caller must align the variables declared and passed
			      C ;				on the appropriate byte boundary (e.g. alignas 64 for 512)
			      C ;				This module is very light-weight (less than 1K bytes) and relatively fast,
			      C ;				but is not intended for all processor types or all environments. 
			      C ;				Use for private (hobbyist), or instructional,
			      C ;				or as an example for more ambitious projects is all it is meant to be.
			      C ;
			      C ;				ui512b provides basic bit-oriented operations: shift left, shift right, and, or, not,
			      C ;               least significant bit and most significant bit.
			      C ;
			      C ;
			      C ;			MIT License
			      C ;
			      C ;			Copyright (c) 2024 John G. Lynch
			      C ;
			      C ;				Permission is hereby granted, free of charge, to any person obtaining a copy
			      C ;				of this software and associated documentation files (the "Software"), to deal
			      C ;				in the Software without restriction, including without limitation the rights
			      C ;				to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
			      C ;				copies of the Software, and to permit persons to whom the Software is
			      C ;				furnished to do so, subject to the following conditions:
			      C ;
			      C ;				The above copyright notice and this permission notice shall be included in all
			      C ;				copies or substantial portions of the Software.
			      C ;
			      C ;				THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
			      C ;				IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
			      C ;				FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
			      C ;				AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
			      C ;				LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
			      C ;				OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
			      C ;				SOFTWARE.
			      C ;
			      C 
			      C IFNDEF						ui512bMacros_INC
 = 1			      C ui512bMacros_INC EQU		<1>
			      C ;           header file equivalent extern declarations
			      C ;			EXTERN "C" signatures (from ui512b.asm)
			      C 
			      C ;   // void shr_u ( u64* destination, u64* source, u32 bits_to_shift )
			      C ;   // shift supplied source 512bit (8 QWORDS) right, put in destination
			      C EXTERNDEF	shr_u:PROC
			      C 
			      C ;   // void shl_u ( u64* destination, u64* source, u16 bits_to_shift );
			      C ;   // shift supplied source 512bit (8 QWORDS) left, put in destination
			      C EXTERNDEF	shl_u:PROC
			      C 
			      C ;   // void and_u ( u64* destination, u64* lh_op, u64* rh_op );
			      C ;   // logical 'AND' bits in lh_op, rh_op, put result in destination
			      C EXTERNDEF	and_u:PROC
			      C 
			      C ;   // logical 'OR' bits in lh_op, rh_op, put result in destination
			      C ;   // void or_u( u64* destination, u64* lh_op, u64* rh_op);
			      C EXTERNDEF	or_u:PROC
			      C 
			      C ;   // logical 'NOT' bits in source, put result in destination
			      C ;	// void not_u( u64* destination, u64* source);
			      C EXTERNDEF	not_u:PROC
			      C 
			      C ;   // find most significant bit in supplied source 512bit (8 QWORDS)
			      C ;	// s16 msb_u( u64* );
			      C ;   // returns: -1 if no most significant bit, bit number otherwise, bits numbered 0 to 511 inclusive
			      C ;			Note:	a returned zero means the significant bit is bit0 of the eighth word of the 512bit source parameter; (the right most bit)
			      C ;					a returned 511 means bit63 of the first word (the left most bit)
			      C EXTERNDEF	msb_u:PROC
			      C 
			      C ;   // find least significant bit in supplied source 512bit (8 QWORDS)
			      C ;	// s16 lsb_u( u64* );
			      C ;   // returns: -1 if no least significant bit, bit number otherwise, bits numbered 0 to 511 inclusive
			      C ;			Note:	a returned zero means the significant bit is bit0 of the eighth word of the 512bit source parameter; (the right most bit)
			      C ;					a returned 511 means bit63 of the first word (the left most bit)
			      C EXTERNDEF	lsb_u:PROC
			      C 
			      C ENDIF
			      C 
								INCLUDE			ui512mdMacros.inc
			      C .NOLIST
			      C ;
			      C ;			ui512mdMacros
			      C ;
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C ;			File:			ui512mdMacros.inc
			      C ;			Author:			John G. Lynch
			      C ;			Legal:			Copyright @2024, per MIT License below
			      C ;			Date:			June 20, 2024
			      C 
			      C ;
			      C ;			Notes:
			      C ;				ui512 is a small project to provide basic operations for a variable type of unsigned 512 bit integer.
			      C ;
			      C ;				ui512a provides basic operations: zero, copy, compare, add, subtract.
			      C ;				ui512b provides basic bit-oriented operations: shift left, shift right, and, or, not, least significant bit and most significant bit.
			      C ;               ui512md provides multiply and divide.
			      C ;
			      C ;				It is written in assembly language, using the MASM (ml64) assembler provided as an option within Visual Studio.
			      C ;				(currently using VS Community 2022 17.9.6)
			      C ;
			      C ;				It provides external signatures that allow linkage to C and C++ programs,
			      C ;				where a shell/wrapper could encapsulate the methods as part of an object.
			      C ;
			      C ;				It has assembly time options directing the use of Intel processor extensions: AVX4, AVX2, SIMD, or none:
			      C ;				(Z (512), Y (256), or X (128) registers, or regular Q (64bit)).
			      C ;
			      C ;				If processor extensions are used, the caller must align the variables declared and passed
			      C ;				on the appropriate byte boundary (e.g. alignas 64 for 512)
			      C ;
			      C ;				This module is very light-weight (less than 1K bytes) and relatively fast,
			      C ;				but is not intended for all processor types or all environments. 
			      C ;
			      C ;				Use for private (hobbyist), or instructional, or as an example for more ambitious projects is all it is meant to be.
			      C ;
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C ;
			      C ;			MIT License
			      C ;
			      C ;			Copyright (c) 2024 John G. Lynch
			      C ;
			      C ;				Permission is hereby granted, free of charge, to any person obtaining a copy
			      C ;				of this software and associated documentation files (the "Software"), to deal
			      C ;				in the Software without restriction, including without limitation the rights
			      C ;				to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
			      C ;				copies of the Software, and to permit persons to whom the Software is
			      C ;				furnished to do so, subject to the following conditions:
			      C ;
			      C ;				The above copyright notice and this permission notice shall be included in all
			      C ;				copies or substantial portions of the Software.
			      C ;
			      C ;				THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
			      C ;				IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
			      C ;				FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
			      C ;				AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
			      C ;				LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
			      C ;				OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
			      C ;				SOFTWARE.
			      C ;
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C .LIST
			      C IFNDEF			ui512mdMacros_INC
 = 1			      C ui512mdMacros_INC EQU			<1>
			      C 
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C ;			signatures (from ui512md.asm)
			      C 
			      C ; //			mult_uT64		-	multiply 512 bit multiplicand by 64 bit multiplier, giving 512 product, 64 bit overflow
			      C ; //			Prototype:		-	void mult_uT64( u64* product, u64* overflow, u64* multiplicand, u64 multiplier);
			      C EXTERNDEF		mult_uT64:PROC	;	void mult_uT64( u64* product, u64* overflow, u64* multiplicand, u64 multiplier);
			      C 
			      C ; //			mult_u			-	multiply 512 multiplicand by 512 multiplier, giving 512 product, overflow
			      C ; //			Prototype:		-	void mult_u( u64* product, u64* overflow, u64* multiplicand, u64* multiplier);
			      C EXTERNDEF		mult_u:PROC		;	void mult_u( u64* product, u64* overflow, u64* multiplicand, u64* multiplier);
			      C 
			      C ; //			div_uT64		-	divide 512 bit dividend by 64 bit bit divisor, giving 512 bit quotient and 64 bit remainder
			      C ; //			Prototype:		-	s16 div_u( u64* quotient, u64* remainder, u64* dividend, u64 divisor,);
			      C EXTERNDEF		div_uT64:PROC	;	s16 div_uT64( u64* quotient, u64* remainder, u64* dividend, u64 divisor);
			      C 
			      C ; //			div_u			-	divide 512 bit dividend by 512 bit divisor, giving 512 bit quotient and remainder
			      C ; //			Prototype:		-	s16 div_u( u64* quotient, u64* remainder, u64* dividend, u64* divisor);
			      C EXTERNDEF		div_u:PROC		;	s16 div_u( u64* quotient, u64* remainder, u64* dividend, u64* divisor);
			      C 
			      C ; //			reg_verify		-	save non-volatile regs for verification (debug)
			      C ; //			Prototype		-	void reg_verify ( u64* reg struct)
			      C EXTERNDEF		reg_verify:PROC	;	void reg_verify ( u64* reg struct)
			      C 
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C 
			      C 
			      C CreateFrame		MACRO			argsize, argsavename
			      C ;
			      C ;			set up frame to save regs, and to create aligned working memory for scratch variables
			      C ;			argsize is the amount of space to be made on the stack for locals and padding (at least 40h on each end)
			      C ;			argsavename is the name of the variable, within the LOCAL declarations, QWORD, for the RBP to be saved, and restored from
			      C ;			No prologue, epilogue should be used. LOCAL declarations immediately after PROC statement, then this macro
			      C ;			Note: LOCAL variables will START on a 64 byte aligned address space. Your order and sizes of your variables will be in sequence as declared
			      C ;			and thus provide no assurance of alignment other than what your declarations imply. In other words, declaring a byte var at the beginning
			      C ;			throws everyting after that off by a byte. OK, not exacly, the asm will align by data type (eg, QWORD on 8 byte), but there is no way to get back
			      C ;			to 64 byte alignment, so declare those first, then QWORD, then DWORD, etc. Or, count your declares such that you get what/where you want.
			      C ;			example:
			      C ;
			      C ;somename	PROC
			      C ;			LOCAL		padding1[8]:QWORD				; warning: do not touch, initialize, or use padding. (on either end)
			      C ;			LOCAL		somelocal:ZMMWORD				; a 512 bit, 64 byte aligned var, ready for aligned load/store into ZMM (AVX2) register
			      C ;			LOCAL		some local variable declarions, some more, and some more
			      C ;			LOCAL		and some more
			      C ;			LOCAL		savedRBP:QWORD					; you might have other reg save space as well
			      C ;			LOCAL		padding2[8]:QWORD				; after your return statement, before ENDP, might LEA padding1 and padding2 to eliminate warning (unreferenced variables)
			      C ;			CREATEFRAME 200h, savedRBP					; (200h assumes 118h in those "some local variable declarations", adjust as necessary)
			      C ;
			      C ;			Use only one return from the PROC, and immediately before return, use ReleaseFrame macro, giving name of where RBP is saved
			      C ;
			      C 				PUSH			RBP
			      C 				MOV				RBP, RSP
			      C 				AND				RSP, -8
			      C 				SUB				RSP, argsize + 64					; make a gap between old stack pointer and current stack pointer for use as "LOCAL", adjust size if changes made to locals
			      C 				MOV				RAX, -64							; Round it (down), to make it, and local vars, align on 64 byte address (necessary for aligned ZMM load/store)
			      C 				AND				RAX, RBP							; note: "padding" variables must not be used. They are in areas where we have just rounded down, or in stack space for those we call
			      C 				XCHG			RAX, RBP							; RBP now points to top of gap between old stack pointer and current stack pointer, aligned on 64
			      C 				MOV				argsavename, RAX					; LOCAL variables now usable, using negative offsets from the new RBP value. Must restore RBP, RSP at exit (Use RELEASEFRAME)
			      C 				ENDM
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C 
			      C 
			      C ReleaseFrame	MACRO			argsavename
			      C ;			release memory set up by createframe macro
			      C ;			restores RSP, and RBP to as-called values
			      C ;			after these instructions are executed, LOCAL variables can NOT be accessed
			      C ;			This needs to be done to restore the stack correctly, but can be done only once
			      C ;			Strongly suggest doing this just prior to return instruction, and that there is only one return instruction from the PROC
			      C 				MOV				RSP, argsavename					; restore unadjusted / unrounded (the as when called) stack pointer (eliminating LOCAL storage) 
			      C 				POP				RBP									; restore base pointer for caller
			      C 				ENDM
			      C 
			      C ;--------------------------------------------------------------------------------------------------------------------------------------------------------------
			      C ;==========================================================================================
			      C ;           Notes on x64 calling conventions        aka "fast call"
			      C ; ref: https://learn.microsoft.com/en-us/cpp/build/x64-calling-convention?view=msvc-170
			      C ; The first four parameters are passed in registers: RCX, RDX, R8, R9 if integer or address
			      C ; if floating point XMM0L, XMM1L, XMM2L, XMM3L
			      C ;===========================================================================================
			      C ;
			      C ;===========================================================================================
			      C ; RAX, RCX, RDX, R8, R9, R10, R11 are considered volatile, and do not need to be saved
			      C ; XMM0, YMM0, ZMM0 and  ..1, ..2, ..3, ..4, and ..5 are considered volatile,
			      C ;	and do not need to be saved
			      C ;  ZMM16 to ZMM31: volatile, also do not need to be zeroed to resume full clock speeds
			      C ;
			      C ; R12, R13, R14, R15, RDI, RSI, RBX, RBP, RSP are non-volatile and if used, must be restored
			      C ; XMM, YMM, and ZMM ..6 thru 15 are non-volatile and if used, must be restored
			      C ;
			      C ; A "leaf" function is one that does not call and does not change non volatile registers
			      C ; leaf functionss therefore do not need frame, prolog or epilog
			      C ;
			      C ;===========================================================================================
			      C 
			      C ENDIF
			      C 

								OPTION			casemap:none
 00000000			.CODE			ui512md
								OPTION			PROLOGUE:none
								OPTION			EPILOGUE:none

								MemConstants
			     1	;
 00000000		     1	zeroQ			DQ				0
	   0000000000000000
 00000008 00000100	     1	mskHex100		DD				0100h
			     1	;		Return codes commonly used.			
 0000000C 00000000	     1	ret0			DD				0								
 00000010 00000001	     1	ret1			DD				1
 00000014 FFFFFFFF	     1	ret_1			DD				-1
			     1	;		Masks commonly used
 00000018 FF		     1	mskAll8			DB				255
 00000019 01		     1	mskB0			DB				1
 0000001A 02		     1	mskB1			DB				2
 0000001B 04		     1	mskB2			DB				4
 0000001C 08		     1	mskB3			DB				8
 0000001D 10		     1	mskB4			DB				16
 0000001E 20		     1	mskB5			DB				32
 0000001F 40		     1	mskB6			DB				64
 00000020 80		     1	mskB7			DB				128
			     1	;
			     1					ALIGN			8									; realign

				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;			EXTERNDEF		mult_u:PROC					; void mult_u( u64* product, u64* overflow, u64* multiplicand, u64* multiplier)
				;			mult_u			-	multiply 512 multiplicand by 512 multiplier, giving 512 product, overflow
				;			Prototype:		-	void mult_u( u64* product, u64* overflow, u64* multiplicand, u64* multiplier);
				;			product			-	Address of 8 QWORDS to store resulting product (in RCX)
				;			overflow		-	Address of 8 QWORDS to store resulting overflow (in RDX)
				;			multiplicand	-	Address of 8 QWORDS multiplicand (in R8)
				;			multiplier		-	Address of 8 QWORDS multiplier (in R9)
				;			returns			-	nothing (0)
				;
 00000028			mult_u			PROC			PUBLIC
								LOCAL			padding1 [ 8 ] : QWORD
								LOCAL			product [ 16 ] : QWORD
								LOCAL			savedRBP : QWORD
								LOCAL			savedRCX : QWORD, savedRDX : QWORD, savedR10 : QWORD, savedR11 : QWORD, savedR12 : QWORD
								LOCAL			plierl : WORD						; low limit index of of multiplier (7 - first non-zero)
								LOCAL			candl : WORD						; low limit index of multiplicand
								LOCAL			padding2 [ 16 ] : QWORD
 = padding2 + 64 - padding1	mult_u_ofs		EQU				padding2 + 64 - padding1			; offset is the size of the local memmory declarations

								CreateFrame		220h, savedRBP
			     1	;
			     1	;			set up frame to save regs, and to create aligned working memory for scratch variables
			     1	;			argsize is the amount of space to be made on the stack for locals and padding (at least 40h on each end)
			     1	;			argsavename is the name of the variable, within the LOCAL declarations, QWORD, for the RBP to be saved, and restored from
			     1	;			No prologue, epilogue should be used. LOCAL declarations immediately after PROC statement, then this macro
			     1	;			Note: LOCAL variables will START on a 64 byte aligned address space. Your order and sizes of your variables will be in sequence as declared
			     1	;			and thus provide no assurance of alignment other than what your declarations imply. In other words, declaring a byte var at the beginning
			     1	;			throws everyting after that off by a byte. OK, not exacly, the asm will align by data type (eg, QWORD on 8 byte), but there is no way to get back
			     1	;			to 64 byte alignment, so declare those first, then QWORD, then DWORD, etc. Or, count your declares such that you get what/where you want.
			     1	;			example:
			     1	;
			     1	;somename	PROC
			     1	;			LOCAL		padding1[8]:QWORD				; warning: do not touch, initialize, or use padding. (on either end)
			     1	;			LOCAL		somelocal:ZMMWORD				; a 512 bit, 64 byte aligned var, ready for aligned load/store into ZMM (AVX2) register
			     1	;			LOCAL		some local variable declarions, some more, and some more
			     1	;			LOCAL		and some more
			     1	;			LOCAL		savedRBP:QWORD					; you might have other reg save space as well
			     1	;			LOCAL		padding2[8]:QWORD				; after your return statement, before ENDP, might LEA padding1 and padding2 to eliminate warning (unreferenced variables)
			     1	;			CREATEFRAME 200h, savedRBP					; (200h assumes 118h in those "some local variable declarations", adjust as necessary)
			     1	;
			     1	;			Use only one return from the PROC, and immediately before return, use ReleaseFrame macro, giving name of where RBP is saved
			     1	;
 00000028  55		     1					PUSH			RBP
 00000029  48/ 8B EC	     1					MOV				RBP, RSP
 0000002C  48/ 83 E4 F8	     1					AND				RSP, -8
 00000030  48/ 81 EC	     1					SUB				RSP, 220h + 64					; make a gap between old stack pointer and current stack pointer for use as "LOCAL", adjust size if changes made to locals
	   00000260
 00000037  48/ C7 C0	     1					MOV				RAX, -64							; Round it (down), to make it, and local vars, align on 64 byte address (necessary for aligned ZMM load/store)
	   FFFFFFC0
 0000003E  48/ 23 C5	     1					AND				RAX, RBP							; note: "padding" variables must not be used. They are in areas where we have just rounded down, or in stack space for those we call
 00000041  48/ 95	     1					XCHG			RAX, RBP							; RBP now points to top of gap between old stack pointer and current stack pointer, aligned on 64
 00000043  48/ 89 85	     1					MOV				savedRBP, RAX					; LOCAL variables now usable, using negative offsets from the new RBP value. Must restore RBP, RSP at exit (Use RELEASEFRAME)
	   FFFFFF38
 0000004A  48/ 89 8D						MOV				savedRCX, RCX
	   FFFFFF30
 00000051  48/ 89 95						MOV				savedRDX, RDX
	   FFFFFF28
 00000058  4C/ 89 95						MOV				savedR10, R10
	   FFFFFF20
 0000005F  4C/ 89 9D						MOV				savedR11, R11
	   FFFFFF18
 00000066  4C/ 89 A5						MOV				savedR12, R12
	   FFFFFF10

								CheckAlign		RCX									; (out) Product
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			RCX, 63							; Is specified param aligned 64?
			     1					JZ				??0000									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0000:
			     1		ENDIF
								CheckAlign		RDX									; (out) Overflow
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			RDX, 63							; Is specified param aligned 64?
			     1					JZ				??0001									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0001:
			     1		ENDIF
								CheckAlign		R8									; (in) Multiplicand
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			R8, 63							; Is specified param aligned 64?
			     1					JZ				??0002									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0002:
			     1		ENDIF
								CheckAlign		R9									; (in) Multiplier
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			R9, 63							; Is specified param aligned 64?
			     1					JZ				??0003									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0003:
			     1		ENDIF

				; Examine multiplicand, save dimensions, handle edge cases of zero or one
 0000006D  49/ 8B C8						MOV				RCX, R8								; examine multiplicand
 00000070  E8 00000000 E					CALL			msb_u								; get count to most significant bit (-1 if no bits)
 00000075  66| 83 F8 FF						CMP				AX, -1								; multiplicand = 0? exit with product = 0
 00000079  0F 84 00000125					JE				@@zeroandexit
 0000007F  66| 83 F8 00						CMP				AX, 0								; multiplicand = 1?	exit with product = multiplier
 00000083  49/ 8B D1						MOV				RDX, R9								; address of multiplier (to be copied to product)
 00000086  0F 84 00000140					JE				@@copyandexit
 0000008C  66| C1 E8 06						SHR				AX, 6								; divide msb by 64 to get Nr words
 00000090  66| B9 0007						MOV				CX, 7 
 00000094  66| 2B C8						SUB				CX, AX								; subtract from 7 to get starting (high order) begining index
 00000097  66| 89 8D						MOV				candl, CX							; save off multiplicand index lower limit (eliminate multiplying leading zero words)
	   FFFFFF0C

				; Examine multiplier, save dimensions, handle edge cases of zero or one
 0000009E  49/ 8B C9						MOV				RCX, R9								; examine multiplier
 000000A1  E8 00000000 E					CALL			msb_u								; get count to most significant bit (-1 if no bits)
 000000A6  66| 83 F8 FF						CMP				AX, -1								; multiplier = 0? exit with product = 0
 000000AA  0F 84 000000F4					JE				@@zeroandexit
 000000B0  66| 83 F8 00						CMP				AX, 0								; multiplier = 1? exit with product = multiplicand
 000000B4  49/ 8B D0						MOV				RDX, R8								; address of multiplicand (to be copied to product)
 000000B7  0F 84 0000010F					JE				@@copyandexit
 000000BD  66| C1 E8 06						SHR				AX, 6
 000000C1  48/ C7 C1						MOV				RCX, 7
	   00000007
 000000C8  66| 2B C8						SUB				CX, AX
 000000CB  66| 89 8D						MOV				plierl, CX							; save off multiplier index lower limit (eliminate multiplying leading zero words)
	   FFFFFF0E

				; In frame / stack reserved memory, clear 16 qword area for working version of overflow/product; set up indexes for loop
 000000D2  48/ 8D 8D						LEA				RCX, product [ 0 ]
	   FFFFFF40
								Zero512			RCX									; clear working copy of overflow, need to start as zero, results are accumulated
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0004									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0004:
			     2		ENDIF
 000000D9  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000000DF  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000000E5  48/ 8D 4D 80						LEA				RCX, product [ 8 * 8 ]
								Zero512			RCX									; clear working copy of product (they need to be contiguous, so using working copy, not callers)
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0005									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0005:
			     2		ENDIF
 000000E9  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000000EF  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000000F5  49/ C7 C3						MOV				R11, 7								; index for multiplier (reduced until less than saved plierl) (outer loop)
	   00000007
 000000FC  4D/ 8B E3						MOV				R12, R11							; index for multiplicand (reduced until less than saved candl) (inner loop)

				; multiply loop: an outer loop for each non-zero qword of multiplicand, with an inner loop for each non-zero qword of multiplier, results accumulated in 'product'
 000000FF			@@multloop:
 000000FF  4D/ 8B D3						MOV				R10, R11							; R10 holds index for overflow / product work area (results)
 00000102  4D/ 03 D4						ADD				R10, R12
 00000105  49/ FF C2						INC				R10									; index for product/overflow 
 00000108  4B/ 8B 04 E0						MOV				RAX, [ R8 ] + [ R12 * 8 ]			; get qword of multiplicand
 0000010C  4B/ F7 24 D9						MUL				Q_PTR [ R9 ] + [ R11 * 8 ]			; multiply by qword of multiplier
 00000110  4A/ 01 84 D5						ADD				product [ R10 * 8 ], RAX			; accummulate in product, this is low-order 64 bits of result of mul
	   FFFFFF40
 00000118  49/ FF CA						DEC				R10									; preserves carry flag
 0000011B			@@:
 0000011B  4A/ 11 94 D5						ADC				product [ R10 * 8 ], RDX			; high-order result of 64bit multiply, plus the carry (if any)
	   FFFFFF40
 00000123  48/ C7 C2						MOV				RDX, 0								; again, preserves carry flag
	   00000000
 0000012A  73 05						JNC				@F									; if adding caused carry, propagate it, else next 
 0000012C  49/ FF CA						DEC				R10									; propagating carry
 0000012F  7D EA						JGE				@B
 00000131			@@:																	; next qword of multiplicand
 00000131  49/ FF CC						DEC				R12
 00000134  66| 44/ 3B A5					CMP				R12W, candl							; Done with inner loop?
	   FFFFFF0C
 0000013C  7D C1						JGE				@@multloop							; no, do it again
 0000013E  49/ C7 C4						MOV				R12, 7								; yes, reset inner loop (multiplicand) index
	   00000007
 00000145  49/ FF CB						DEC				R11									; decrement index for outer loop
 00000148  66| 44/ 3B 9D					CMP				R11W, plierl						; done with outer loop?
	   FFFFFF0E
 00000150  7D AD						JGE				@@multloop							; no, do it again with next qword of multiplier

				; finished: copy working product/overflow to callers product / overflow
 00000152  48/ 8B 8D						MOV				RCX, savedRCX
	   FFFFFF30
 00000159  48/ 8D 55 80						LEA				RDX, product [ 8 * 8 ]
								Copy512			RCX, RDX							; copy working product to callers product
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0006									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0006:
			     2		ENDIF
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??0007									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0007:
			     2		ENDIF
 0000015D  62 61 FD 48/ 6F   1					VMOVDQA64		ZMM31, ZM_PTR [ RDX ]
	   3A
 00000163  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					VMOVDQA64		YMM4, YM_PTR [ RDX + 0 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 0 * 8 ], YMM4	; alternate ymm regs in case pipeline can execute next without waiting for this.
			     1					VMOVDQA64		YMM5, YM_PTR [ RDX + 4 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 4 * 8 ], YMM5
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1	
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 0 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 0 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 2 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 2 * 8 ], XMM3
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 4 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 4 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 6 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 6 * 8 ], XMM3
			     1		ELSE
			     1					FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				RAX, Q_PTR [ RDX + idx * 8 ]
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 00000169  48/ 8B 8D						MOV				RCX, savedRDX
	   FFFFFF28
 00000170  48/ 8D 95						LEA				RDX, product [ 0 ]
	   FFFFFF40
								Copy512			RCX, RDX							; copy working overflow to callers overflow
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0008									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0008:
			     2		ENDIF
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??0009									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0009:
			     2		ENDIF
 00000177  62 61 FD 48/ 6F   1					VMOVDQA64		ZMM31, ZM_PTR [ RDX ]
	   3A
 0000017D  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					VMOVDQA64		YMM4, YM_PTR [ RDX + 0 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 0 * 8 ], YMM4	; alternate ymm regs in case pipeline can execute next without waiting for this.
			     1					VMOVDQA64		YMM5, YM_PTR [ RDX + 4 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 4 * 8 ], YMM5
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1	
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 0 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 0 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 2 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 2 * 8 ], XMM3
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 4 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 4 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 6 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 6 * 8 ], XMM3
			     1		ELSE
			     1					FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				RAX, Q_PTR [ RDX + idx * 8 ]
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF

				; restore regs, release frame, return
 00000183			@@exit:			
 00000183  4C/ 8B 95						MOV				R10, savedR10
	   FFFFFF20
 0000018A  4C/ 8B 9D						MOV				R11, savedR11
	   FFFFFF18
 00000191  4C/ 8B A5						MOV				R12, savedR12						; restore any non-volitile regs used
	   FFFFFF10
								ReleaseFrame	savedRBP
			     1	;			release memory set up by createframe macro
			     1	;			restores RSP, and RBP to as-called values
			     1	;			after these instructions are executed, LOCAL variables can NOT be accessed
			     1	;			This needs to be done to restore the stack correctly, but can be done only once
			     1	;			Strongly suggest doing this just prior to return instruction, and that there is only one return instruction from the PROC
 00000198  48/ 8B A5	     1					MOV				RSP, savedRBP					; restore unadjusted / unrounded (the as when called) stack pointer (eliminating LOCAL storage) 
	   FFFFFF38
 0000019F  5D		     1					POP				RBP									; restore base pointer for caller
 000001A0  48/ 33 C0						XOR				RAX, RAX							; return zero
								RET
 000001A3  C3		   *	    ret    00000h

				; zero callers product and overflow
 000001A4			@@zeroandexit:
 000001A4  48/ 8B 8D						MOV				RCX, savedRCX						; reload address of callers product
	   FFFFFF30
								Zero512			RCX									; zero it
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??000A									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??000A:
			     2		ENDIF
 000001AB  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000001B1  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000001B7  48/ 8B 8D						MOV				RCX, savedRDX						; reload address of caller overflow
	   FFFFFF28
								Zero512			RCX									; zero it
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??000B									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??000B:
			     2		ENDIF
 000001BE  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000001C4  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000001CA  EB B7						JMP				@@exit

				; multiplying by 1: zero overflow, copy the non-one to the product
 000001CC			@@copyandexit:
 000001CC  48/ 8B 8D						MOV				RCX, savedRDX						; address of passed overflow
	   FFFFFF28
								Zero512			RCX 								; zero it
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??000C									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??000C:
			     2		ENDIF
 000001D3  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000001D9  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000001DF  48/ 8B 8D						MOV				RCX, savedRCX						; copy (whichever: multiplier or multiplicand) to callers product
	   FFFFFF30
								Copy512			RCX, RDX							; RDX "passed" here from whomever jumped here (either multiplier, or multiplicand in RDX)
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??000D									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??000D:
			     2		ENDIF
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??000E									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??000E:
			     2		ENDIF
 000001E6  62 61 FD 48/ 6F   1					VMOVDQA64		ZMM31, ZM_PTR [ RDX ]
	   3A
 000001EC  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					VMOVDQA64		YMM4, YM_PTR [ RDX + 0 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 0 * 8 ], YMM4	; alternate ymm regs in case pipeline can execute next without waiting for this.
			     1					VMOVDQA64		YMM5, YM_PTR [ RDX + 4 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 4 * 8 ], YMM5
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1	
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 0 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 0 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 2 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 2 * 8 ], XMM3
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 4 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 4 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 6 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 6 * 8 ], XMM3
			     1		ELSE
			     1					FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				RAX, Q_PTR [ RDX + idx * 8 ]
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000001F2  EB 8F						JMP				@@exit								; and exit
 000001F4			mult_u			ENDP

				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;			EXTERNDEF		mult_uT64:PROC				;	void mult_uT64( u64* product, u64* overflow, u64* multiplicand, u64 multiplier);
				;			mult_uT64		-	multiply 512 bit multiplicand by 64 bit multiplier, giving 512 product, 64 bit overflow
				;			Prototype:		-	void mult_uT64( u64* product, u64* overflow, u64* multiplicand, u64 multiplier);
				;			product			-	Address of 8 QWORDS to store resulting product (in RCX)
				;			overflow		-	Address of QWORD for resulting overflow (in RDX)
				;			multiplicand	-	Address of 8 QWORDS multiplicand (in R8)
				;			multiplier		-	multiplier QWORD (in R9)
				;			returns			-	nothing (0)

								OPTION			PROLOGUE:none
								OPTION			EPILOGUE:none
 000001F4			mult_uT64		PROC			PUBLIC
								
								CheckAlign		RCX									; (out) Product
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			RCX, 63							; Is specified param aligned 64?
			     1					JZ				??000F									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??000F:
			     1		ENDIF
								CheckAlign		R8									; (in) Multiplicand
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			R8, 63							; Is specified param aligned 64?
			     1					JZ				??0010									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0010:
			     1		ENDIF

				; caller might be doing multiply 'in-place', so need to save the original multiplicand, prior to clearing callers product (A = A * x), or (A *= x)
								FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
								PUSH			Q_PTR [ R8 ] + [ idx * 8 ]
								ENDM
 000001F4  41/ FF 30	     1					PUSH			Q_PTR [ R8 ] + [ 0 * 8 ]
 000001F7  41/ FF 70 08	     1					PUSH			Q_PTR [ R8 ] + [ 1 * 8 ]
 000001FB  41/ FF 70 10	     1					PUSH			Q_PTR [ R8 ] + [ 2 * 8 ]
 000001FF  41/ FF 70 18	     1					PUSH			Q_PTR [ R8 ] + [ 3 * 8 ]
 00000203  41/ FF 70 20	     1					PUSH			Q_PTR [ R8 ] + [ 4 * 8 ]
 00000207  41/ FF 70 28	     1					PUSH			Q_PTR [ R8 ] + [ 5 * 8 ]
 0000020B  41/ FF 70 30	     1					PUSH			Q_PTR [ R8 ] + [ 6 * 8 ]
 0000020F  41/ FF 70 38	     1					PUSH			Q_PTR [ R8 ] + [ 7 * 8 ]

				; clear callers product and overflow
				;	Note: if caller used multiplicand and product as the same variable (memory space),
				;	this would wipe the multiplicand. Hence the saving of the multiplicand on the stack. (above)
 00000213  48/ 33 C0						XOR				RAX, RAX
								Zero512			RCX, "Q"   							; clear callers product (multiply uses an additive carry, so it needs to start zeroed)
			     1		IF		__UseZ AND "Q" NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			ZMM31, ZMM31, ZMM31
			     1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
			     1		ELSEIF	__UseY AND "Q" NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND "Q" NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
 00000216  48/ 33 C0	     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
 00000219  48/ 89 01	     2					MOV				Q_PTR [ RCX + 0 * 8 ], RAX
 0000021C  48/ 89 41 08	     2					MOV				Q_PTR [ RCX + 1 * 8 ], RAX
 00000220  48/ 89 41 10	     2					MOV				Q_PTR [ RCX + 2 * 8 ], RAX
 00000224  48/ 89 41 18	     2					MOV				Q_PTR [ RCX + 3 * 8 ], RAX
 00000228  48/ 89 41 20	     2					MOV				Q_PTR [ RCX + 4 * 8 ], RAX
 0000022C  48/ 89 41 28	     2					MOV				Q_PTR [ RCX + 5 * 8 ], RAX
 00000230  48/ 89 41 30	     2					MOV				Q_PTR [ RCX + 6 * 8 ], RAX
 00000234  48/ 89 41 38	     2					MOV				Q_PTR [ RCX + 7 * 8 ], RAX
			     1		ENDIF
 00000238  48/ 89 02						MOV				Q_PTR [ RDX ], RAX					; clear callers overflow
 0000023B  4C/ 8B D2		 				MOV				R10, RDX							; RDX (pointer to callers overflow) gets used in the MUL: save it in R10

				; FOR EACH index of 7 thru 1 (omiting 0): fetch (pop) qword of multiplicand, multiply, add 128 bit result (RAX, RDX) to running working product
								FOR				idx, < 7, 6, 5, 4, 3, 2, 1 >		; Note: this is not a 'real' for, this is a macro that generates an unwound loop
								POP				RAX									; multiplicand [ idx ] qword -> RAX
								MUL				R9									; times multiplier -> RAX, RDX
								ADD				Q_PTR [ RCX ] + [ idx * 8 ], RAX	; add RAX to working product [ idx ] qword
								ADC				Q_PTR [ RCX ] + [ (idx - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
								ENDM
 0000023E  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 0000023F  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 00000242  48/ 01 41 38	     1					ADD				Q_PTR [ RCX ] + [ 7 * 8 ], RAX	; add RAX to working product [ idx ] qword
 00000246  48/ 11 51 30	     1					ADC				Q_PTR [ RCX ] + [ (7 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
 0000024A  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 0000024B  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 0000024E  48/ 01 41 30	     1					ADD				Q_PTR [ RCX ] + [ 6 * 8 ], RAX	; add RAX to working product [ idx ] qword
 00000252  48/ 11 51 28	     1					ADC				Q_PTR [ RCX ] + [ (6 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
 00000256  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 00000257  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 0000025A  48/ 01 41 28	     1					ADD				Q_PTR [ RCX ] + [ 5 * 8 ], RAX	; add RAX to working product [ idx ] qword
 0000025E  48/ 11 51 20	     1					ADC				Q_PTR [ RCX ] + [ (5 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
 00000262  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 00000263  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 00000266  48/ 01 41 20	     1					ADD				Q_PTR [ RCX ] + [ 4 * 8 ], RAX	; add RAX to working product [ idx ] qword
 0000026A  48/ 11 51 18	     1					ADC				Q_PTR [ RCX ] + [ (4 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
 0000026E  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 0000026F  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 00000272  48/ 01 41 18	     1					ADD				Q_PTR [ RCX ] + [ 3 * 8 ], RAX	; add RAX to working product [ idx ] qword
 00000276  48/ 11 51 10	     1					ADC				Q_PTR [ RCX ] + [ (3 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
 0000027A  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 0000027B  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 0000027E  48/ 01 41 10	     1					ADD				Q_PTR [ RCX ] + [ 2 * 8 ], RAX	; add RAX to working product [ idx ] qword
 00000282  48/ 11 51 08	     1					ADC				Q_PTR [ RCX ] + [ (2 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product
 00000286  58		     1					POP				RAX									; multiplicand [ idx ] qword -> RAX
 00000287  49/ F7 E1	     1					MUL				R9									; times multiplier -> RAX, RDX
 0000028A  48/ 01 41 08	     1					ADD				Q_PTR [ RCX ] + [ 1 * 8 ], RAX	; add RAX to working product [ idx ] qword
 0000028E  48/ 11 11	     1					ADC				Q_PTR [ RCX ] + [ (1 - 1) * 8 ], RDX	; and add RDX with carry to [ idx - 1 ] qword of working product

				; Most significant (idx=0), the high order result of the multiply in RDX, goes to the overflow of the caller
 00000291  58							POP				RAX
 00000292  49/ F7 E1						MUL				R9
 00000295  48/ 01 01						ADD				Q_PTR [ RCX ] + [ 0 * 8 ], RAX
 00000298  49/ 11 12						ADC				Q_PTR [ R10 ], RDX						; last qword overflow is also the operation overflow
 0000029B  48/ 33 C0						XOR				RAX, RAX							; return zero
 0000029E  C3							RET
 0000029F			mult_uT64		ENDP

				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;			EXTERNDEF		div_u:PROC					; s16 div_u( u64* quotient, u64* remainder, u64* dividend, u64* divisor)
				;			div_u			-	divide 512 bit dividend by 512 bit divisor, giving 512 bit quotient and remainder
				;			Prototype:		-	s16 div_u( u64* quotient, u64* remainder, u64* dividend, u64* divisor);
				;			quotient		-	Address of 8 QWORDS to store resulting quotient (in RCX)
				;			remainder		-	Address of 8 QWORDs for resulting remainder (in RDX)
				;			dividend		-	Address of 8 QWORDS dividend (in R8)
				;			divisor			-	Address of 8 QWORDs divisor (in R9)
				;			returns			-	0 for success, -1 for attempt to divide by zero

								OPTION			PROLOGUE:none
								OPTION			EPILOGUE:none
 0000029F			div_u			PROC			PUBLIC

								LOCAL			padding1 [ 16 ] : QWORD
								LOCAL			currnumerator [ 16 ] : QWORD
								LOCAL			qdiv [ 16 ] : QWORD, quotient [ 8 ] : QWORD, normdivisor [ 8 ] : QWORD
								LOCAL			savedRCX : QWORD, savedRDX : QWORD, savedR8 : QWORD, savedR9 : QWORD
								LOCAL			savedR10 : QWORD, savedR11 : QWORD, savedR12 : QWORD, savedRBP : QWORD
								LOCAL			qHat : QWORD, rHat : QWORD,	nDiv : QWORD, addbackRDX : QWORD, addbackR11 : QWORD
								LOCAL			normf : WORD, jIdx : WORD, mIdx : WORD, nIdx : WORD, mDim : WORD, nDim : Word			
								LOCAL			padding2 [ 16 ] : QWORD
 = padding2 + 64 - padding1	div_oset		EQU				padding2 + 64 - padding1

								CreateFrame		360h, savedRBP
			     1	;
			     1	;			set up frame to save regs, and to create aligned working memory for scratch variables
			     1	;			argsize is the amount of space to be made on the stack for locals and padding (at least 40h on each end)
			     1	;			argsavename is the name of the variable, within the LOCAL declarations, QWORD, for the RBP to be saved, and restored from
			     1	;			No prologue, epilogue should be used. LOCAL declarations immediately after PROC statement, then this macro
			     1	;			Note: LOCAL variables will START on a 64 byte aligned address space. Your order and sizes of your variables will be in sequence as declared
			     1	;			and thus provide no assurance of alignment other than what your declarations imply. In other words, declaring a byte var at the beginning
			     1	;			throws everyting after that off by a byte. OK, not exacly, the asm will align by data type (eg, QWORD on 8 byte), but there is no way to get back
			     1	;			to 64 byte alignment, so declare those first, then QWORD, then DWORD, etc. Or, count your declares such that you get what/where you want.
			     1	;			example:
			     1	;
			     1	;somename	PROC
			     1	;			LOCAL		padding1[8]:QWORD				; warning: do not touch, initialize, or use padding. (on either end)
			     1	;			LOCAL		somelocal:ZMMWORD				; a 512 bit, 64 byte aligned var, ready for aligned load/store into ZMM (AVX2) register
			     1	;			LOCAL		some local variable declarions, some more, and some more
			     1	;			LOCAL		and some more
			     1	;			LOCAL		savedRBP:QWORD					; you might have other reg save space as well
			     1	;			LOCAL		padding2[8]:QWORD				; after your return statement, before ENDP, might LEA padding1 and padding2 to eliminate warning (unreferenced variables)
			     1	;			CREATEFRAME 200h, savedRBP					; (200h assumes 118h in those "some local variable declarations", adjust as necessary)
			     1	;
			     1	;			Use only one return from the PROC, and immediately before return, use ReleaseFrame macro, giving name of where RBP is saved
			     1	;
 0000029F  55		     1					PUSH			RBP
 000002A0  48/ 8B EC	     1					MOV				RBP, RSP
 000002A3  48/ 83 E4 F8	     1					AND				RSP, -8
 000002A7  48/ 81 EC	     1					SUB				RSP, 360h + 64					; make a gap between old stack pointer and current stack pointer for use as "LOCAL", adjust size if changes made to locals
	   000003A0
 000002AE  48/ C7 C0	     1					MOV				RAX, -64							; Round it (down), to make it, and local vars, align on 64 byte address (necessary for aligned ZMM load/store)
	   FFFFFFC0
 000002B5  48/ 23 C5	     1					AND				RAX, RBP							; note: "padding" variables must not be used. They are in areas where we have just rounded down, or in stack space for those we call
 000002B8  48/ 95	     1					XCHG			RAX, RBP							; RBP now points to top of gap between old stack pointer and current stack pointer, aligned on 64
 000002BA  48/ 89 85	     1					MOV				savedRBP, RAX					; LOCAL variables now usable, using negative offsets from the new RBP value. Must restore RBP, RSP at exit (Use RELEASEFRAME)
	   FFFFFDC0
 000002C1  48/ 89 8D						MOV				savedRCX, RCX
	   FFFFFDF8
 000002C8  48/ 89 95						MOV				savedRDX, RDX
	   FFFFFDF0
 000002CF  4C/ 89 85						MOV				savedR8, R8
	   FFFFFDE8
 000002D6  4C/ 89 8D						MOV				savedR9, R9
	   FFFFFDE0
 000002DD  4C/ 89 95						MOV				savedR10, R10
	   FFFFFDD8
 000002E4  4C/ 89 9D						MOV				savedR11, R11
	   FFFFFDD0
 000002EB  4C/ 89 A5						MOV				savedR12, R12
	   FFFFFDC8

								CheckAlign		RCX									; (out) Quotient
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			RCX, 63							; Is specified param aligned 64?
			     1					JZ				??0011									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0011:
			     1		ENDIF
								CheckAlign		RDX									; (out) Remainder
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			RDX, 63							; Is specified param aligned 64?
			     1					JZ				??0012									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0012:
			     1		ENDIF
								CheckAlign		R8									; (in) Dividend
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			R8, 63							; Is specified param aligned 64?
			     1					JZ				??0013									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0013:
			     1		ENDIF
								CheckAlign		R9									; (in) Divisor
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			R9, 63							; Is specified param aligned 64?
			     1					JZ				??0014									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0014:
			     1		ENDIF

				; Initialize
								Zero512			RCX									; zero callers quotient
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0015									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0015:
			     2		ENDIF
 000002F2  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000002F8  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
								Zero512			RDX									; zero callers remainder
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??0016									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0016:
			     2		ENDIF
 000002FE  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 00000304  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RDX ], ZMM31
	   3A
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RDX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RDX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RDX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 0000030A  48/ 8D 8D						LEA				RCX, quotient
	   FFFFFE40
								Zero512			RCX									; zero working quotient
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0017									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0017:
			     2		ENDIF
 00000311  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 00000317  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF

				 ; Examine divisor
 0000031D  49/ 8B C9						MOV				RCX, R9								; divisor
 00000320  E8 00000000 E					CALL			msb_u								; get most significant bit
 00000325  66| 83 F8 FF						CMP				AX, -1								; msb < 0? 
 00000329  0F 84 0000029A					JE				divbyzero							; divisor is zero, abort
 0000032F  66| 83 F8 40						CMP				AX, 64								; divisor only one 64-bit word?
 00000333  EB 44						JMP				mbynDiv								; no, do divide of m digit by n digit				*** NOTE: restore JGE after testing (this allows full div on single word)

				;	divide of m 64-bit qwords by one 64 bit qword divisor, use the quicker divide routine (div_uT64), and return
 00000335  48/ 8B 8D						MOV				RCX, savedRCX						; set up parms for call to div by 64bit: RCX - addr of quotient
	   FFFFFDF8
 0000033C  48/ 8B 95						MOV				RDX, savedRDX						; RDX - addr of remainder
	   FFFFFDF0
 00000343  4C/ 8B 85						MOV				R8, savedR8							; R8 - addr of dividend
	   FFFFFDE8
 0000034A  48/ 8B 85						MOV				RAX, savedR9
	   FFFFFDE0
 00000351  4C/ 8B 48 38						MOV				R9, Q_PTR [ RAX ] + [ 7 * 8 ]		; R9 - value of 64 bit divisor
 00000355  E8 00000293						CALL			div_uT64
 0000035A  48/ 8B 95						MOV				RDX, savedRDX						; move 64 bit remainder to last word of 8 word remainder
	   FFFFFDF0
 00000361  48/ 8B 0A						MOV				RCX, Q_PTR [ RDX ]					; get the one qword remainder
								Zero512			RDX									; clear the 8 qword callers remainder
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??0018									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0018:
			     2		ENDIF
 00000364  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 0000036A  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RDX ], ZMM31
	   3A
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RDX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RDX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RDX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 00000370  48/ 89 4A 38						MOV				Q_PTR [ RDX ] + [ 7 * 8 ], RCX		; put the one qword remainder in the least significant qword of the callers remainder
 00000374  E9 00000213						JMP				cleanupret

				;
				; Going to divide an 'm' digit dividend (u), by an 'n' digit divisor (v)
				;	See Knuth, The Art of Computer Programming, Volume 2, Algorithm D, Pages 272-278
				;	Notes: This is much like the long division done by hand as taught in school, but instead of digits being 0 to 9, they are 0 to 2^64, or whole 64 bit qwords.
				;	Knuth suggests any base can work, but suggests the selection of one more 'natural' to the machine. Since x64 machines have a 128-bit by 64 bit divide instruction,
				;	The selected base 'b' is one whole qword, or 64 bits, or 2^64. As in manual division, the first non-obvious step is to 'align' the leading bits of the divisor. Knuth calls this
				;	'normalization'. It requires determining the dimensions of the variables, and the most significant bit of the divisor. Both variables are then shifted (left) to get the 
				;	most significant bit of the divisor into the most significant bit of the qword in which it is found. This makes the leading digit of the divisor greter than or equal to 2^64 / 2,
				;	making the first division meaningful. The dividend must then be shifted the same amount.
				;	When the division is completed, no action needs to be taken on the quotient - it will be correct without shifting back (the shifts of dividend and divisor cancel each other out).
				;	The remainder will need to be shifted (right) to "de-normalize" for the return value.
				;	This process yields the dimensions of the variables: the number of qwords in each: 'm' for dividend, 'n' for divisor. Midx, and Nidx are the starting indexes, reflecting m and n.
				;
 00000379			mbynDiv:
 00000379  66| 89 85						MOV				nDim, AX							; still have divisor msb in AX
	   FFFFFD8C
 00000380  66| C1 AD						SHR				nDim, 6								; div msb by 64 to get msq (most significant qword) aka 'n' (zero based Nr qwords)
	   FFFFFD8C 06
 00000388  66| B9 0007						MOV				CX, 7
 0000038C  66| 2B 8D						SUB				CX, nDim
	   FFFFFD8C
 00000393  66| FF 85						INC				nDim
	   FFFFFD8C
 0000039A  66| 89 8D						MOV				nIdx, CX							; nIdx now 7 - msb of leading word: aka idx to first qword
	   FFFFFD90
								;
 000003A1  66| 89 85						MOV				normf, AX
	   FFFFFD96
 000003A8  66| 83 A5						AND				normf, 63
	   FFFFFD96 3F
 000003B0  66| B8 003F						MOV				AX, 63
 000003B4  66| 2B 85						SUB				AX, normf							; Nr bits to get leading divisor bit to msb saved at normf
	   FFFFFD96
 000003BB  66| 89 85						MOV				normf, AX
	   FFFFFD96
																					
				;	Notes: (continued from above) The mIdx and nIdx are indexes pointing the the leading non-zero qword in dividend and divisor respectively.
				;	jIDx is the number of non-zero qwords in the dividend (after normalization shift). It starts at first non-zero qword

				; Step D1: Normalize	
								
 000003C2  48/ 8B 95						MOV				RDX, savedR9						; callers divisor
	   FFFFFDE0
 000003C9  48/ 8D 8D						LEA				RCX, normdivisor					; local copy of divisor, normalized
	   FFFFFE00
 000003D0  66| 44/ 8B 85					MOV				R8W, normf
	   FFFFFD96
 000003D8  E8 00000000 E					CALL			shl_u								; by shifting until MSB is high bit								
								;
 000003DD  48/ 8B 95						MOV				RDX, savedR8						; callers dividend
	   FFFFFDE8
 000003E4  48/ 8D 8D						LEA				RCX, currnumerator [ 8 * 8 ]		; starting numerator is the normalized supplied dividend
	   FFFFFF40
 000003EB  66| 44/ 8B 85					MOV				R8W, normf							; shifted the same Nr bits as it took to make divisor msb the leading bit
	   FFFFFD96
 000003F3  E8 00000000 E					CALL			shl_u
 000003F8  48/ 8D 95						LEA				RDX, currnumerator					; zero first eight words of working enumerator
	   FFFFFF00
								Zero512			RDX
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??0019									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0019:
			     2		ENDIF
 000003FF  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 00000405  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RDX ], ZMM31
	   3A
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RDX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RDX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RDX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 0000040B  48/ 8B 95						MOV				RDX, savedR8
	   FFFFFDE8
 00000412  48/ 8B 02						MOV				RAX, Q_PTR [ RDX ]					; if numerator has 8 qwords (dimM = 7), then the shift just lost us high order bits, get them			
 00000415  66| B9 003F						MOV				CX, 63								; shift first qword of dividend right 
 00000419  66| 2B 8D						SUB				CX, normf							; by 63 minus Nr bits shifted left
	   FFFFFD96
 00000420  48/ D3 E8						SHR				RAX, CL								; eliminating all bit the bits we lost
 00000423  4C/ 8D A5						LEA				R12, currnumerator [ 7 * 8 ]
	   FFFFFF38
 0000042A  49/ 89 04 24						MOV				[ R12 ], RAX						; store them at m + 1 (the word 'before' the other up to eight qwords)
								;
 0000042E  48/ 33 C0						XOR				RAX, RAX
 00000431  48/ C7 C1						MOV				RCX, -1
	   FFFFFFFF
 00000438			@@:
 00000438  49/ 85 0C C4						TEST			Q_PTR [ R12 ] + [ RAX * 8 ], RCX	; R12 currently has addr of begining of enumerator, 
 0000043C  75 09						JNZ				@F
 0000043E  48/ FF C0						INC				RAX
 00000441  48/ 83 F8 08						CMP				RAX, 8
 00000445  7E F1						JLE				@B
 00000447			@@:
 00000447  4D/ 8D 24 C4						LEA				R12, Q_PTR [ R12 ] + [ RAX * 8 ]	; revise begining address of working numerator with address of first non-zero qword
 0000044B  48/ 8D 0C 25						LEA				RCX, [ 9 ]
	   00000009
 00000453  66| 2B C8						SUB				CX, AX
 00000456  66| 89 8D						MOV				mDim, CX
	   FFFFFD8E
 0000045D  66| 89 85						MOV				mIdx, AX							; mIdx after normalize
	   FFFFFD92
 00000464  66| 89 85						MOV				jIdx, AX							; loop counter (Nr words in denominator)
	   FFFFFD94
 0000046B  66| 3B 8D						CMP				CX, nDim 
	   FFFFFD8C
 00000472  0F 8C 00000159					JL				numtoremain							; if dimension M (dividend) is less than dimension N (divisor), exit with result zero
 00000478  48/ 0F B7 85						MOVZX			RAX, nIdx							; first word of divisor
	   FFFFFD90
 00000480  48/ 8B 84 C5						MOV				RAX, normdivisor [ RAX * 8 ]
	   FFFFFE00
 00000488  48/ 89 85						MOV				nDiv, RAX							; save and re-use first qword of divisor (used each time to determine qhat)
	   FFFFFDA8
 0000048F  4C/ 8D A5						LEA				R12, currnumerator [ 7 * 8 ]
	   FFFFFF38

				;			Step D3: Calculate  q^
 00000496			D3:
 00000496  48/ 0F B7 8D						MOVZX			RCX, jIdx
	   FFFFFD94
 0000049E  49/ 8B 14 CC						MOV				RDX, Q_PTR [ R12 ] + [ RCX * 8 ]
 000004A2  48/ FF C1						INC				RCX
 000004A5  49/ 8B 04 CC						MOV				RAX, Q_PTR [ R12 ] + [ RCX * 8 ]
 000004A9  48/ F7 B5						DIV				nDiv
	   FFFFFDA8
 000004B0  48/ 89 85						MOV				qHat, RAX
	   FFFFFDB8
 000004B7  48/ 89 95						MOV				rHat, RDX
	   FFFFFDB0

				;			Step D4: Multiply trial quotient digit by full normalized divisor, then subtract from working copy of numerator (tricky alignment issues)
 000004BE			D4:
 000004BE  48/ 8D 8D						LEA				RCX, qdiv
	   FFFFFE80
								Zero512			RCX
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??001A									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??001A:
			     2		ENDIF
 000004C5  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 000004CB  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000004D1  48/ 8D 8D						LEA				RCX, qdiv [ 8 * 8 ]					; qdiv is 16 qwords. Last eight are answer
	   FFFFFEC0
 000004D8  48/ 8D 95						LEA				RDX, qdiv [ 7 * 8 ]					; Nr 7 will be overflow (if any)
	   FFFFFEB8
 000004DF  4C/ 8D 85						LEA				R8, normdivisor						; the normalized 8 qword divisor
	   FFFFFE00
 000004E6  4C/ 8B 8D						MOV				R9, qHat							; times qhat
	   FFFFFDB8
 000004ED  E8 FFFFFD02						CALL			mult_uT64							; multiply divisor by trial quotient digit (qHat)

						; a little intricate here: 	Need the starting address of the two operands, the shorter of the two (remaining) lengths
						; expaining the intricacies: the leading digit of the multiplied number needs to line up with the leading digit of the enumerator @ [ R12 ]
						; the multiply may have added a digit (a qword). The added digit may be in the answer, or may be in the overflow
						; subtract the result of the multiply from the remaining (current) numerator

				;			Step D5: Test remainder
 000004F2			D5:
 000004F2  48/ 0F B7 85						MOVZX			RAX, jIdx
	   FFFFFD94
 000004FA  48/ 8B 8D						MOV				RCX, qHat
	   FFFFFDB8
 00000501  48/ 89 8C C5						MOV				quotient [ RAX * 8 ], RCX			; Set quotient digit [ j ] 
	   FFFFFE40
 00000509  73 37						JNC				D7									; Carry (from above) indicates result of subtract went negative, need D6 add back, else on to D7

				;			Step D6: Add Back
 0000050B			D6:
 0000050B  48/ FF 8C C5						DEC				quotient [ RAX * 8 ]				; adjust quotient digit
	   FFFFFE40
 00000513  48/ 8B 95						MOV				RDX, addbackRDX						; restore same indexes / counters used in subtract for add back
	   FFFFFDA0
 0000051A  4C/ 8B 95						MOV				R10, addbackR11
	   FFFFFD98
 00000521  F8							CLC													; Carry indicator
 00000522			@@:
 00000522  49/ 8B 04 D3						MOV				RAX, Q_PTR [ R11 ] + [ RDX * 8 ]
 00000526  49/ 8D 4C 24						LEA				RCX, Q_PTR [ R12 ] + [ 1 * 8 ]
	   08
 0000052B  48/ 11 04 D1						ADC				Q_PTR [ RCX ] + [ RDX * 8 ], RAX
 0000052F  48/ FF CA						DEC				RDX
 00000532  49/ FF CA						DEC				R10
 00000535  79 EB						JNS				@B
 00000537  48/ 8B 85						MOV				RAX, qdiv [ 7 * 8 ]
	   FFFFFEB8
 0000053E  49/ 11 04 D4						ADC				Q_PTR [ R12 ] + [ RDX * 8 ], RAX

				;			Step D7: Loop on j
 00000542			D7:
 00000542  66| FF 85						INC				jIdx								; increase loop counter
	   FFFFFD94
 00000549  66| 83 BD						CMP				jIdx, 8								; done?
	   FFFFFD94 08
 00000551  0F 8E FFFFFF3F					JLE				D3									; no, loop to D3

				;			Step D8: Un Normalize:
 00000557			D8UnNormalize:
 00000557  48/ 8B 8D						MOV				RCX, savedRDX						; reduced working numerator is now the remainder
	   FFFFFDF0
 0000055E  48/ 8D 95						LEA				RDX, currnumerator + [ 8 * 8 ]		; shifted result to callers remainder
	   FFFFFF40
 00000565  66| 44/ 8B 85					MOV				R8W, normf
	   FFFFFD96
 0000056D  E8 00000000 E					CALL			shr_u
								;
 00000572  48/ 8B 8D						MOV				RCX, savedRCX						; copy working quotient to callers quotient
	   FFFFFDF8
 00000579  48/ 8D 95						LEA				RDX, quotient
	   FFFFFE40
								Copy512			RCX, RDX
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??001B									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??001B:
			     2		ENDIF
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??001C									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??001C:
			     2		ENDIF
 00000580  62 61 FD 48/ 6F   1					VMOVDQA64		ZMM31, ZM_PTR [ RDX ]
	   3A
 00000586  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					VMOVDQA64		YMM4, YM_PTR [ RDX + 0 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 0 * 8 ], YMM4	; alternate ymm regs in case pipeline can execute next without waiting for this.
			     1					VMOVDQA64		YMM5, YM_PTR [ RDX + 4 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RCX + 4 * 8 ], YMM5
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1	
			     1					CheckAlign		RCX
			     1					CheckAlign		RDX
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 0 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 0 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 2 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 2 * 8 ], XMM3
			     1					MOVDQA			XMM4, XM_PTR [ RDX + 4 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 4 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ RDX + 6 * 8 ]
			     1					MOVDQA			XM_PTR [ RCX + 6 * 8 ], XMM3
			     1		ELSE
			     1					FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				RAX, Q_PTR [ RDX + idx * 8 ]
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 0000058C			cleanupret:
 0000058C  48/ 33 C0						XOR				RAX, RAX							; return zero
 0000058F			cleanupwretcode:
 0000058F  4C/ 8B A5						MOV				R12, savedR12
	   FFFFFDC8
 00000596  4C/ 8B 9D						MOV				R11, savedR11
	   FFFFFDD0
 0000059D  4C/ 8B 95						MOV				R10, savedR10
	   FFFFFDD8
 000005A4  4C/ 8B 8D						MOV				R9,  savedR9
	   FFFFFDE0
 000005AB  4C/ 8B 85						MOV				R8,  savedR8
	   FFFFFDE8
 000005B2  48/ 8B 95						MOV				RDX, savedRDX
	   FFFFFDF0
 000005B9  48/ 8B 8D						MOV				RCX, savedRCX						; restore parameter registers back to "as-called" values
	   FFFFFDF8
								ReleaseFrame	savedRBP
			     1	;			release memory set up by createframe macro
			     1	;			restores RSP, and RBP to as-called values
			     1	;			after these instructions are executed, LOCAL variables can NOT be accessed
			     1	;			This needs to be done to restore the stack correctly, but can be done only once
			     1	;			Strongly suggest doing this just prior to return instruction, and that there is only one return instruction from the PROC
 000005C0  48/ 8B A5	     1					MOV				RSP, savedRBP					; restore unadjusted / unrounded (the as when called) stack pointer (eliminating LOCAL storage) 
	   FFFFFDC0
 000005C7  5D		     1					POP				RBP									; restore base pointer for caller
								RET
 000005C8  C3		   *	    ret    00000h
 000005C9			divbyzero:
 000005C9  8B 05 00000014 R					MOV				EAX, ret_1
 000005CF  EB BE						JMP				cleanupwretcode

 000005D1  4C/ 8B 85		numtoremain:	MOV				R8, savedR8
	   FFFFFDE8
 000005D8  48/ 8B 95						MOV				RDX, savedRDX
	   FFFFFDF0
								Copy512			RDX, R8
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RDX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RDX, 63							; Is specified param aligned 64?
			     2					JZ				??001D									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??001D:
			     2		ENDIF
			     1					CheckAlign		R8
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			R8, 63							; Is specified param aligned 64?
			     2					JZ				??001E									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??001E:
			     2		ENDIF
 000005DF  62 41 FD 48/ 6F   1					VMOVDQA64		ZMM31, ZM_PTR [ R8 ]
	   38
 000005E5  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RDX ], ZMM31
	   3A
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RDX
			     1					CheckAlign		R8
			     1					VMOVDQA64		YMM4, YM_PTR [ R8 + 0 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RDX + 0 * 8 ], YMM4	; alternate ymm regs in case pipeline can execute next without waiting for this.
			     1					VMOVDQA64		YMM5, YM_PTR [ R8 + 4 * 8 ]
			     1					VMOVDQA64		YM_PTR [ RDX + 4 * 8 ], YMM5
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1	
			     1					CheckAlign		RDX
			     1					CheckAlign		R8
			     1					MOVDQA			XMM4, XM_PTR [ R8 + 0 * 8 ]
			     1					MOVDQA			XM_PTR [ RDX + 0 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ R8 + 2 * 8 ]
			     1					MOVDQA			XM_PTR [ RDX + 2 * 8 ], XMM3
			     1					MOVDQA			XMM4, XM_PTR [ R8 + 4 * 8 ]
			     1					MOVDQA			XM_PTR [ RDX + 4 * 8 ], XMM4
			     1					MOVDQA			XMM3, XM_PTR [ R8 + 6 * 8 ]
			     1					MOVDQA			XM_PTR [ RDX + 6 * 8 ], XMM3
			     1		ELSE
			     1					FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				RAX, Q_PTR [ R8 + idx * 8 ]
			     1					MOV				Q_PTR [ RDX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 000005EB  EB 9F						JMP				cleanupret

 000005ED			div_u			ENDP

				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;			EXTERNDEF		div_uT64:PROC				; s16 div_uT64( u64* quotient, u64* remainder, u64* dividend, u64 divisor)
				;			div_uT64		-	divide 512 bit dividend by 64 bit divisor, giving 512 bit quotient and 64 bit remainder
				;			Prototype:		-	s16 div_u( u64* quotient, u64* remainder, u64* dividend, u64 divisor);
				;			quotient		-	Address of 8 QWORDS to store resulting quotient (in RCX)
				;			remainder		-	Address of QWORD for resulting remainder (in RDX)
				;			dividend		-	Address of 8 QWORDS dividend (in R8)
				;			divisor			-	Value of 64 bit divisor (in R9)
				;			returns			-	0 for success, -1 for attempt to divide by zero
				;
				;			Regs with contents destroyed, not restored: RAX, RDX, R10 (each considered volitile, but caller might optimize on other regs)

								OPTION			PROLOGUE:none
								OPTION			EPILOGUE:none
 000005ED			div_uT64		PROC			PUBLIC

								CheckAlign		RCX									; (out) Quotient
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			RCX, 63							; Is specified param aligned 64?
			     1					JZ				??001F									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??001F:
			     1		ENDIF
								CheckAlign		R8									; (in) Dividend
			     1					LOCAL			ok
			     1		IF	__CheckAlign
			     1					TEST			R8, 63							; Is specified param aligned 64?
			     1					JZ				??0020									; Yes, passes test, continue
			     1					INT				0									; No? fails, break (can substitute other exception handling)
			     1	??0020:
			     1		ENDIF

				; Test divisor for divide by zero				
 000005ED  4D/ 85 C9						TEST			R9, R9
 000005F0  74 63						JZ				@@DivByZero

				; DIV instruction (64-bit) uses RAX and RDX. Need to move RDX (addr of remainder) out of the way; start it off with zero
 000005F2  4C/ 8B D2						MOV				R10, RDX
 000005F5  48/ 33 D2						XOR				RDX, RDX

				; FOR EACH index of 0 thru 7: get qword of dividend, divide by divisor, store qword of quotient
								FOR				idx, < 0, 1, 2, 3, 4, 5, 6, 7 >
								MOV				RAX, Q_PTR [ R8 ] + [ idx * 8 ]	; dividend [ idx ] -> RAX
								DIV				R9									; divide by divisor in R9 (as passed)
								MOV				Q_PTR [ RCX ] + [ idx * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
								ENDM
 000005F8  49/ 8B 00	     1					MOV				RAX, Q_PTR [ R8 ] + [ 0 * 8 ]	; dividend [ idx ] -> RAX
 000005FB  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 000005FE  48/ 89 01	     1					MOV				Q_PTR [ RCX ] + [ 0 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 00000601  49/ 8B 40 08	     1					MOV				RAX, Q_PTR [ R8 ] + [ 1 * 8 ]	; dividend [ idx ] -> RAX
 00000605  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 00000608  48/ 89 41 08	     1					MOV				Q_PTR [ RCX ] + [ 1 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 0000060C  49/ 8B 40 10	     1					MOV				RAX, Q_PTR [ R8 ] + [ 2 * 8 ]	; dividend [ idx ] -> RAX
 00000610  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 00000613  48/ 89 41 10	     1					MOV				Q_PTR [ RCX ] + [ 2 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 00000617  49/ 8B 40 18	     1					MOV				RAX, Q_PTR [ R8 ] + [ 3 * 8 ]	; dividend [ idx ] -> RAX
 0000061B  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 0000061E  48/ 89 41 18	     1					MOV				Q_PTR [ RCX ] + [ 3 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 00000622  49/ 8B 40 20	     1					MOV				RAX, Q_PTR [ R8 ] + [ 4 * 8 ]	; dividend [ idx ] -> RAX
 00000626  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 00000629  48/ 89 41 20	     1					MOV				Q_PTR [ RCX ] + [ 4 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 0000062D  49/ 8B 40 28	     1					MOV				RAX, Q_PTR [ R8 ] + [ 5 * 8 ]	; dividend [ idx ] -> RAX
 00000631  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 00000634  48/ 89 41 28	     1					MOV				Q_PTR [ RCX ] + [ 5 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 00000638  49/ 8B 40 30	     1					MOV				RAX, Q_PTR [ R8 ] + [ 6 * 8 ]	; dividend [ idx ] -> RAX
 0000063C  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 0000063F  48/ 89 41 30	     1					MOV				Q_PTR [ RCX ] + [ 6 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide
 00000643  49/ 8B 40 38	     1					MOV				RAX, Q_PTR [ R8 ] + [ 7 * 8 ]	; dividend [ idx ] -> RAX
 00000647  49/ F7 F1	     1					DIV				R9									; divide by divisor in R9 (as passed)
 0000064A  48/ 89 41 38	     1					MOV				Q_PTR [ RCX ] + [ 7 * 8 ], RAX	; quotient [ idx ] <- RAX ; Note: remainder in RDX for next divide

				; Last (least significant qword) divide leaves a remainder, store it at callers remainder
 0000064E  49/ 89 12						MOV				Q_PTR [ R10 ], RDX					; remainder to callers remainder
 00000651  48/ 33 C0						XOR				RAX, RAX							; return zero
 00000654			@@exit:			
 00000654  C3							RET

				; Exception handling, divide by zero
 00000655			@@DivByZero:
								Zero512			RCX									; Divide by Zero. Could throw fault, but returning zero quotient, zero remainder
			     1		IF		__UseZ AND " " NE "Q"
			     1					CheckAlign		RCX
			     2					LOCAL			ok
			     2		IF	__CheckAlign
			     2					TEST			RCX, 63							; Is specified param aligned 64?
			     2					JZ				??0021									; Yes, passes test, continue
			     2					INT				0									; No? fails, break (can substitute other exception handling)
			     2	??0021:
			     2		ENDIF
 00000655  62 01 85 40/ EF   1					VPXORQ			ZMM31, ZMM31, ZMM31
	   FF
 0000065B  62 61 FD 48/ 7F   1					VMOVDQA64		ZM_PTR [ RCX ], ZMM31
	   39
			     1		ELSEIF	__UseY AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					VPXORQ			YMM4, YMM4, YMM4
			     1					FOR				idx, < 0, 4 >
			     1					VMOVDQA64		YM_PTR [ RCX + idx * 8 ], YMM4
			     1					ENDM
			     1		ELSEIF	__UseX AND " " NE "Q"
			     1					CheckAlign		RCX
			     1					PXOR			XMM4, XMM4
			     1					FOR				idx, < 0, 2, 4, 6 >
			     1					MOVDQA			XM_PTR [ RCX + idx * 8 ], XMM4
			     1					ENDM		
			     1		ELSE
			     1					XOR				RAX, RAX
			     1					FOR				idx,  < 0, 1, 2, 3, 4, 5, 6, 7 >
			     1					MOV				Q_PTR [ RCX + idx * 8 ], RAX
			     1					ENDM
			     1		ENDIF
 00000661  48/ 33 C0						XOR				RAX, RAX
 00000664  49/ 89 02						MOV				Q_PTR [ R10 ] , RAX
 00000667  FF C8						DEC				EAX									; return error (div by zero)
 00000669  EB E9						JMP				@@exit
 0000066B			div_uT64		ENDP


				;
				;--------------------------------------------------------------------------------------------------------------------------------------------------------------
				;			EXTERNDEF		reg_verify:PROC	;	void reg_verify ( u64* reg struct)
				;			div_uT64		-	divide 512 bit dividend by 64 bit divisor, giving 512 bit quotient and 64 bit remainder
								OPTION			PROLOGUE:none
								OPTION			EPILOGUE:none
 0000066B			reg_verify		PROC			PUBLIC

 0000066B  4C/ 89 21						MOV				Q_PTR [ RCX ] + [ 0 * 8 ], R12
 0000066E  4C/ 89 69 08						MOV				Q_PTR [ RCX ] + [ 1 * 8 ], R13
 00000672  4C/ 89 71 10						MOV				Q_PTR [ RCX ] + [ 2 * 8 ], R14
 00000676  4C/ 89 79 18						MOV				Q_PTR [ RCX ] + [ 3 * 8 ], R15
 0000067A  48/ 89 79 20						MOV				Q_PTR [ RCX ] + [ 4 * 8 ], RDI
 0000067E  48/ 89 71 28						MOV				Q_PTR [ RCX ] + [ 5 * 8 ], RSI
 00000682  48/ 89 59 30						MOV				Q_PTR [ RCX ] + [ 6 * 8 ], RBX
 00000686  48/ 89 69 38						MOV				Q_PTR [ RCX ] + [ 7 * 8 ], RBP
 0000068A  48/ 89 61 40						MOV				Q_PTR [ RCX ] + [ 8 * 8 ], RSP

 0000068E  C3							RET

 0000068F			reg_verify		ENDP
								END
Microsoft (R) Macro Assembler (x64) Version 14.44.35207.1   05/26/25 13:24:26
ui512md.asm						     Symbols 2 - 1




Macros:

                N a m e                 Type

CheckAlign . . . . . . . . . . .	Proc
Copy512  . . . . . . . . . . . .	Proc
CreateFrame  . . . . . . . . . .	Proc
GetZatMask . . . . . . . . . . .	Proc
MemConstants . . . . . . . . . .	Proc
ReleaseFrame . . . . . . . . . .	Proc
SetZatMask . . . . . . . . . . .	Proc
Zero512  . . . . . . . . . . . .	Proc


Records:

                N a m e                  Width     # fields
                                         Shift     Width     Mask      Initial

kMask  . . . . . . . . . . . . .	 00000009      00000009
  b8 . . . . . . . . . . . . . .	 00000008      00000001	     0100     ?
  b7 . . . . . . . . . . . . . .	 00000007      00000001	     0080     ?
  b6 . . . . . . . . . . . . . .	 00000006      00000001	     0040     ?
  b5 . . . . . . . . . . . . . .	 00000005      00000001	     0020     ?
  b4 . . . . . . . . . . . . . .	 00000004      00000001	     0010     ?
  b3 . . . . . . . . . . . . . .	 00000003      00000001	     0008     ?
  b2 . . . . . . . . . . . . . .	 00000002      00000001	     0004     ?
  b1 . . . . . . . . . . . . . .	 00000001      00000001	     0002     ?
  b0 . . . . . . . . . . . . . .	 00000000      00000001	     0001     ?


Segments:

                N a m e                  Length   Align   Class

ui512md  . . . . . . . . . . . .	 0000068F 16	  'CODE'	


Procedures, parameters, and locals:

                N a m e                 Type     Value    Attr

div_uT64 . . . . . . . . . . . .	P 	 000005ED ui512md	Length= 0000007E Public
  @@exit . . . . . . . . . . . .	L 	 00000654 ui512md	
  @@DivByZero  . . . . . . . . .	L 	 00000655 ui512md	
div_u  . . . . . . . . . . . . .	P 	 0000029F ui512md	Length= 0000034E Public
  padding1 . . . . . . . . . . .	QWord	 rbp - 00000080
  currnumerator  . . . . . . . .	QWord	 rbp - 00000100
  qdiv . . . . . . . . . . . . .	QWord	 rbp - 00000180
  quotient . . . . . . . . . . .	QWord	 rbp - 000001C0
  normdivisor  . . . . . . . . .	QWord	 rbp - 00000200
  savedRCX . . . . . . . . . . .	QWord	 rbp - 00000208
  savedRDX . . . . . . . . . . .	QWord	 rbp - 00000210
  savedR8  . . . . . . . . . . .	QWord	 rbp - 00000218
  savedR9  . . . . . . . . . . .	QWord	 rbp - 00000220
  savedR10 . . . . . . . . . . .	QWord	 rbp - 00000228
  savedR11 . . . . . . . . . . .	QWord	 rbp - 00000230
  savedR12 . . . . . . . . . . .	QWord	 rbp - 00000238
  savedRBP . . . . . . . . . . .	QWord	 rbp - 00000240
  qHat . . . . . . . . . . . . .	QWord	 rbp - 00000248
  rHat . . . . . . . . . . . . .	QWord	 rbp - 00000250
  nDiv . . . . . . . . . . . . .	QWord	 rbp - 00000258
  addbackRDX . . . . . . . . . .	QWord	 rbp - 00000260
  addbackR11 . . . . . . . . . .	QWord	 rbp - 00000268
  normf  . . . . . . . . . . . .	Word	 rbp - 0000026A
  jIdx . . . . . . . . . . . . .	Word	 rbp - 0000026C
  mIdx . . . . . . . . . . . . .	Word	 rbp - 0000026E
  nIdx . . . . . . . . . . . . .	Word	 rbp - 00000270
  mDim . . . . . . . . . . . . .	Word	 rbp - 00000272
  nDim . . . . . . . . . . . . .	Word	 rbp - 00000274
  padding2 . . . . . . . . . . .	QWord	 rbp - 000002F4
  mbynDiv  . . . . . . . . . . .	L 	 00000379 ui512md	
  D3 . . . . . . . . . . . . . .	L 	 00000496 ui512md	
  D4 . . . . . . . . . . . . . .	L 	 000004BE ui512md	
  D5 . . . . . . . . . . . . . .	L 	 000004F2 ui512md	
  D6 . . . . . . . . . . . . . .	L 	 0000050B ui512md	
  D7 . . . . . . . . . . . . . .	L 	 00000542 ui512md	
  D8UnNormalize  . . . . . . . .	L 	 00000557 ui512md	
  cleanupret . . . . . . . . . .	L 	 0000058C ui512md	
  cleanupwretcode  . . . . . . .	L 	 0000058F ui512md	
  divbyzero  . . . . . . . . . .	L 	 000005C9 ui512md	
  numtoremain  . . . . . . . . .	L 	 000005D1 ui512md	
mult_uT64  . . . . . . . . . . .	P 	 000001F4 ui512md	Length= 000000AB Public
mult_u . . . . . . . . . . . . .	P 	 00000028 ui512md	Length= 000001CC Public
  padding1 . . . . . . . . . . .	QWord	 rbp - 00000040
  product  . . . . . . . . . . .	QWord	 rbp - 000000C0
  savedRBP . . . . . . . . . . .	QWord	 rbp - 000000C8
  savedRCX . . . . . . . . . . .	QWord	 rbp - 000000D0
  savedRDX . . . . . . . . . . .	QWord	 rbp - 000000D8
  savedR10 . . . . . . . . . . .	QWord	 rbp - 000000E0
  savedR11 . . . . . . . . . . .	QWord	 rbp - 000000E8
  savedR12 . . . . . . . . . . .	QWord	 rbp - 000000F0
  plierl . . . . . . . . . . . .	Word	 rbp - 000000F2
  candl  . . . . . . . . . . . .	Word	 rbp - 000000F4
  padding2 . . . . . . . . . . .	QWord	 rbp - 00000174
  @@multloop . . . . . . . . . .	L 	 000000FF ui512md	
  @@exit . . . . . . . . . . . .	L 	 00000183 ui512md	
  @@zeroandexit  . . . . . . . .	L 	 000001A4 ui512md	
  @@copyandexit  . . . . . . . .	L 	 000001CC ui512md	
reg_verify . . . . . . . . . . .	P 	 0000066B ui512md	Length= 00000024 Public


Symbols:

                N a m e                 Type     Value    Attr

B_PTR  . . . . . . . . . . . . .	Text   	 BYTE PTR
CPEQ . . . . . . . . . . . . . .	Number	 00000000h   
CPFALSE  . . . . . . . . . . . .	Number	 00000003h   
CPGE . . . . . . . . . . . . . .	Number	 00000005h   
CPGT . . . . . . . . . . . . . .	Number	 00000006h   
CPLE . . . . . . . . . . . . . .	Number	 00000002h   
CPLT . . . . . . . . . . . . . .	Number	 00000001h   
CPNE . . . . . . . . . . . . . .	Number	 00000004h   
CPTRUE . . . . . . . . . . . . .	Number	 00000007h   
D_PTR  . . . . . . . . . . . . .	Text   	 DWORD PTR
MaskBit0 . . . . . . . . . . . .	Number	 00000001h   
MaskBit1 . . . . . . . . . . . .	Number	 00000002h   
MaskBit2 . . . . . . . . . . . .	Number	 00000004h   
MaskBit3 . . . . . . . . . . . .	Number	 00000008h   
MaskBit4 . . . . . . . . . . . .	Number	 00000010h   
MaskBit5 . . . . . . . . . . . .	Number	 00000020h   
MaskBit6 . . . . . . . . . . . .	Number	 00000040h   
MaskBit7 . . . . . . . . . . . .	Number	 00000080h   
Q_PTR  . . . . . . . . . . . . .	Text   	 QWORD PTR
W_PTR  . . . . . . . . . . . . .	Text   	 WORD PTR
XM_PTR . . . . . . . . . . . . .	Text   	 XMMWORD PTR
YM_PTR . . . . . . . . . . . . .	Text   	 YMMWORD PTR
ZM_PTR . . . . . . . . . . . . .	Text   	 ZMMWORD PTR
__CheckAlign . . . . . . . . . .	Number	 00000000h   
__UseQ . . . . . . . . . . . . .	Number	 00000001h   
__UseX . . . . . . . . . . . . .	Number	 00000000h   
__UseY . . . . . . . . . . . . .	Number	 00000000h   
__UseZ . . . . . . . . . . . . .	Number	 00000001h   
add_uT64 . . . . . . . . . . . .	L 	 00000000 External
add_u  . . . . . . . . . . . . .	L 	 00000000 External
and_u  . . . . . . . . . . . . .	L 	 00000000 External
compare_uT64 . . . . . . . . . .	L 	 00000000 External
compare_u  . . . . . . . . . . .	L 	 00000000 External
copy_u . . . . . . . . . . . . .	L 	 00000000 External
div_oset . . . . . . . . . . . .	Text   	 padding2 + 64 - padding1
lsb_u  . . . . . . . . . . . . .	L 	 00000000 External
m32BCST  . . . . . . . . . . . .	Text   	 DWORD BCST
m64BCST  . . . . . . . . . . . .	Text   	 QWORD BCST
msb_u  . . . . . . . . . . . . .	L 	 00000000 External
mskAll8  . . . . . . . . . . . .	Byte	 00000018 ui512md	
mskB0  . . . . . . . . . . . . .	Byte	 00000019 ui512md	
mskB1  . . . . . . . . . . . . .	Byte	 0000001A ui512md	
mskB2  . . . . . . . . . . . . .	Byte	 0000001B ui512md	
mskB3  . . . . . . . . . . . . .	Byte	 0000001C ui512md	
mskB4  . . . . . . . . . . . . .	Byte	 0000001D ui512md	
mskB5  . . . . . . . . . . . . .	Byte	 0000001E ui512md	
mskB6  . . . . . . . . . . . . .	Byte	 0000001F ui512md	
mskB7  . . . . . . . . . . . . .	Byte	 00000020 ui512md	
mskHex100  . . . . . . . . . . .	DWord	 00000008 ui512md	
mult_u_ofs . . . . . . . . . . .	Text   	 padding2 + 64 - padding1
not_u  . . . . . . . . . . . . .	L 	 00000000 External
or_u . . . . . . . . . . . . . .	L 	 00000000 External
ret0 . . . . . . . . . . . . . .	DWord	 0000000C ui512md	
ret1 . . . . . . . . . . . . . .	DWord	 00000010 ui512md	
ret_1  . . . . . . . . . . . . .	DWord	 00000014 ui512md	
set_uT64 . . . . . . . . . . . .	L 	 00000000 External
shl_u  . . . . . . . . . . . . .	L 	 00000000 External
shr_u  . . . . . . . . . . . . .	L 	 00000000 External
sub_uT64 . . . . . . . . . . . .	L 	 00000000 External
sub_u  . . . . . . . . . . . . .	L 	 00000000 External
ui512aMacros_INC . . . . . . . .	Text   	 1
ui512bMacros_INC . . . . . . . .	Text   	 1
ui512mdMacros_INC  . . . . . . .	Text   	 1
zeroQ  . . . . . . . . . . . . .	QWord	 00000000 ui512md	
zero_u . . . . . . . . . . . . .	L 	 00000000 External

	   0 Warnings
	   0 Errors
